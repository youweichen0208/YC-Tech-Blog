# ğŸ¤– æœ¬åœ°å¤§æ¨¡å‹é›†æˆç³»ç»Ÿæ¶æ„

> åŸºäº Mac M2 èŠ¯ç‰‡çš„æœ¬åœ°å¤§æ¨¡å‹éƒ¨ç½²ä¸ Claude Tools é›†æˆæ–¹æ¡ˆ

## ğŸ“– æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„AIå·¥å…·é“¾æ¶æ„ï¼Œå°† Claude Code çš„å¼ºå¤§å·¥å…·èƒ½åŠ›ä¸æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°æˆæœ¬æ•ˆç›Šæœ€å¤§åŒ–ã€éšç§ä¿æŠ¤å’Œæ€§èƒ½ä¼˜åŒ–çš„å®Œç¾å¹³è¡¡ã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·äº¤äº’å±‚"
        A[Claude Code CLI] --> B[ä»»åŠ¡åˆ†æå™¨]
    end

    subgraph "è·¯ç”±å†³ç­–å±‚"
        B --> C{æ™ºèƒ½è·¯ç”±å™¨}
        C -->|å¤æ‚æ¨ç†| D[Claude API]
        C -->|æœ¬åœ°å¤„ç†| E[æœ¬åœ°å¤§æ¨¡å‹è°ƒç”¨]
    end

    subgraph "æœ¬åœ°æœåŠ¡å±‚"
        E --> F[è½¬å‘ä»£ç†æœåŠ¡<br/>FastAPI :8000]
        F --> G[Ollama æœåŠ¡<br/>:11434]
    end

    subgraph "æ¨¡å‹å±‚"
        G --> H[Llama 3.1 8B<br/>é€šç”¨ä»»åŠ¡]
        G --> I[Qwen 2.5 7B<br/>ä¸­æ–‡ä¼˜åŒ–]
        G --> J[DeepSeek Coder 6.7B<br/>ä»£ç ä»»åŠ¡]
    end

    subgraph "åŸºç¡€è®¾æ–½"
        K[Mac M2 èŠ¯ç‰‡<br/>ç»Ÿä¸€å†…å­˜æ¶æ„] --> L[Metal GPU åŠ é€Ÿ]
        L --> G
    end

    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style F fill:#e8f5e8
    style G fill:#fff3e0
    style K fill:#fce4ec
```

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### ğŸ’° æˆæœ¬ä¼˜åŒ–
- **Claude API**: ~$3-15/ç™¾ä¸‡ tokens
- **æœ¬åœ°æ¨¡å‹**: ä»…æ¶ˆè€—ç”µè´¹ (~0.1å…ƒ/å°æ—¶)
- **é€‚ç”¨åœºæ™¯**: é‡å¤æ€§ä»»åŠ¡ã€å¼€å‘æµ‹è¯•ã€æ‰¹é‡å¤„ç†

### ğŸ”’ éšç§ä¿æŠ¤
- æ•æ„Ÿä»£ç æ— éœ€ä¸Šä¼ äº‘ç«¯
- ç§æœ‰æ•°æ®æœ¬åœ°å¤„ç†
- ç¬¦åˆä¼ä¸šå®‰å…¨è¦æ±‚

### âš¡ æ€§èƒ½è¡¨ç°
- M2 èŠ¯ç‰‡åŸç”Ÿä¼˜åŒ–
- GPU åŠ é€Ÿæ¨ç†
- 8B æ¨¡å‹å“åº”æ—¶é—´ ~2s
- æ”¯æŒå¹¶å‘è¯·æ±‚å¤„ç†

## ğŸ”„ æ™ºèƒ½è·¯ç”±ç­–ç•¥

### ä»»åŠ¡åˆ†ç±»å†³ç­–æ ‘

```mermaid
flowchart TD
    A[æ¥æ”¶ä»»åŠ¡] --> B{å¤æ‚åº¦è¯„ä¼°}
    B -->|é«˜å¤æ‚åº¦| C[ä½¿ç”¨ Claude API]
    B -->|ä¸­ä½å¤æ‚åº¦| D{ä»»åŠ¡ç±»å‹è¯†åˆ«}

    D -->|ä»£ç ç›¸å…³| E[DeepSeek Coder 6.7B]
    D -->|ä¸­æ–‡å¤„ç†| F[Qwen 2.5 7B]
    D -->|é€šç”¨ä»»åŠ¡| G[Llama 3.1 8B]
    D -->|ç‰¹æ®Šéœ€æ±‚| H[åŠ¨æ€æ¨¡å‹é€‰æ‹©]

    C --> I[é«˜è´¨é‡è¾“å‡º]
    E --> J[ä»£ç ä¸“ä¸šè¾“å‡º]
    F --> K[ä¸­æ–‡ä¼˜åŒ–è¾“å‡º]
    G --> L[é€šç”¨è´¨é‡è¾“å‡º]
    H --> M[å®šåˆ¶åŒ–è¾“å‡º]

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

### è·¯ç”±è§„åˆ™é…ç½®

| ä»»åŠ¡ç±»å‹ | å¤æ‚åº¦ | æ¨èæ¨¡å‹ | ä½¿ç”¨åœºæ™¯ |
|---------|--------|----------|----------|
| ä»£ç å®¡æŸ¥ | ä½-ä¸­ | DeepSeek Coder | æœ¬åœ°ä»£ç å®‰å…¨æ£€æŸ¥ |
| æ–‡æœ¬ç¿»è¯‘ | ä½ | Qwen 2.5 | ä¸­è‹±æ–‡äº’è¯‘ |
| æ–‡æ¡£æ‘˜è¦ | ä½-ä¸­ | Llama 3.1 | æ‰¹é‡æ–‡æ¡£å¤„ç† |
| åˆ›æ„å†™ä½œ | é«˜ | Claude API | é«˜è´¨é‡å†…å®¹åˆ›ä½œ |
| å¤æ‚æ¨ç† | é«˜ | Claude API | é€»è¾‘åˆ†æã€ç­–ç•¥åˆ¶å®š |

## ğŸ› ï¸ æŠ€æœ¯å®ç°æ ˆ

### æ ¸å¿ƒç»„ä»¶
- **Ollama**: æœ¬åœ°æ¨¡å‹è¿è¡Œæ—¶
- **FastAPI**: é«˜æ€§èƒ½ API ä»£ç†æœåŠ¡
- **httpx**: å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
- **Claude Tools**: å·¥å…·è°ƒç”¨æ¡†æ¶

### æ¨¡å‹é€‰æ‹©ç­–ç•¥
```python
MODEL_RECOMMENDATIONS = {
    "mac_m2_8gb": ["llama3.1:8b", "qwen2.5:7b"],
    "mac_m2_16gb": ["llama3.1:8b", "qwen2.5:14b", "deepseek-coder:6.7b"],
    "mac_m2_24gb": ["llama3.1:70b-q4", "qwen2.5:32b", "deepseek-coder:33b"]
}
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### å“åº”æ—¶é—´å¯¹æ¯”
| ä»»åŠ¡ç±»å‹ | æœ¬åœ°æ¨¡å‹ (8B) | Claude API | æˆæœ¬å¯¹æ¯” |
|---------|-------------|-----------|---------|
| ç®€å•é—®ç­” | 2-3s | 1-2s | 100:1 |
| ä»£ç è§£é‡Š | 3-5s | 2-3s | 50:1 |
| æ–‡æ¡£ç¿»è¯‘ | 1-2s | 1-2s | 200:1 |
| æ–‡æœ¬æ‘˜è¦ | 2-4s | 1-3s | 150:1 |

### èµ„æºæ¶ˆè€—ç›‘æ§
```bash
# å†…å­˜ä½¿ç”¨ç›‘æ§
ollama ps

# GPU ä½¿ç”¨ç‡æŸ¥çœ‹
sudo powermetrics -n 1 --samplers gpu_power

# æ¨¡å‹åˆ‡æ¢å»¶è¿Ÿ
time ollama run llama3.1:8b "æµ‹è¯•å“åº”"
```

## ğŸ”§ éƒ¨ç½²æ¶æ„è¯¦è§£

### 1. åŸºç¡€ç¯å¢ƒå‡†å¤‡
```bash
# ç³»ç»Ÿè¦æ±‚æ£€æŸ¥
system_profiler SPHardwareDataType | grep "Apple M"
vm_stat | grep "free"

# ä¼˜åŒ–è®¾ç½®
export OLLAMA_GPU_LAYERS=99
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=3
```

### 2. æœåŠ¡ç¼–æ’
```yaml
# docker-compose.yml (å¯é€‰)
version: '3.8'
services:
  local-llm-proxy:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  ollama_data:
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# setup-local-llm.sh

echo "ğŸš€ å¼€å§‹éƒ¨ç½²æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ..."

# 1. å®‰è£… Ollama
if ! command -v ollama &> /dev/null; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# 2. ä¸‹è½½æ¨èæ¨¡å‹
echo "ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹..."
ollama pull llama3.1:8b
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b

# 3. å®‰è£… Python ä¾èµ–
echo "ğŸ æ­£åœ¨å®‰è£… Python ä¾èµ–..."
pip install fastapi uvicorn httpx pydantic

# 4. å¯åŠ¨æœåŠ¡
echo "ğŸ”¥ æ­£åœ¨å¯åŠ¨æœåŠ¡..."
python local-llm-proxy.py &

# 5. å¥åº·æ£€æŸ¥
sleep 5
if curl -s http://localhost:8000/health | grep -q "ok"; then
    echo "âœ… ç³»ç»Ÿéƒ¨ç½²æˆåŠŸï¼"
    echo "ğŸŒ ä»£ç†æœåŠ¡: http://localhost:8000"
    echo "ğŸ¤– Ollama æœåŠ¡: http://localhost:11434"
else
    echo "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
fi
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯1: éšç§ä»£ç å®¡æŸ¥
```python
# ç§æœ‰ä»£ç æœ¬åœ°å®¡æŸ¥
import httpx

async def review_private_code(code_content):
    response = await httpx.post(
        "http://localhost:8000/api/generate",
        json={
            "prompt": f"è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç çš„å®‰å…¨æ¼æ´ï¼š\n{code_content}",
            "model": "deepseek-coder:6.7b",
            "temperature": 0.3
        }
    )
    return response.json()["response"]
```

### åœºæ™¯2: æ‰¹é‡æ–‡æ¡£å¤„ç†
```bash
# æ‰¹é‡ç¿»è¯‘æ–‡æ¡£
for file in docs/*.md; do
    echo "å¤„ç†æ–‡ä»¶: $file"
    curl -X POST http://localhost:8000/api/generate \
        -H "Content-Type: application/json" \
        -d "{
            \"prompt\": \"å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ï¼š\\n$(cat $file)\",
            \"model\": \"qwen2.5:7b\"
        }"
done
```

### åœºæ™¯3: å¼€å‘åŠ©æ‰‹é›†æˆ
```typescript
// Claude Tools é›†æˆç¤ºä¾‹
class LocalLLMTool {
    async call(prompt: string, options = {}) {
        const defaultOptions = {
            model: "llama3.1:8b",
            temperature: 0.7,
            maxTokens: 2000
        };

        const config = { ...defaultOptions, ...options };

        const response = await fetch('http://localhost:8000/api/generate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                prompt,
                ...config
            })
        });

        return response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const llm = new LocalLLMTool();
const result = await llm.call("è§£é‡ŠReact Hooksçš„å·¥ä½œåŸç†", {
    model: "deepseek-coder:6.7b"
});
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### æ¨¡å‹å‚æ•°ä¼˜åŒ–
```python
# é’ˆå¯¹ä¸åŒä»»åŠ¡çš„å‚æ•°è°ƒä¼˜
TASK_CONFIGS = {
    "code_generation": {
        "temperature": 0.1,
        "top_p": 0.9,
        "repeat_penalty": 1.1
    },
    "creative_writing": {
        "temperature": 0.8,
        "top_p": 0.95,
        "repeat_penalty": 1.0
    },
    "translation": {
        "temperature": 0.3,
        "top_p": 0.9,
        "repeat_penalty": 1.05
    }
}
```

### ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_llm_call(prompt_hash: str, model: str):
    # ç¼“å­˜ç›¸åŒè¯·æ±‚çš„ç»“æœ
    return call_ollama(prompt, model)

def get_prompt_hash(prompt: str) -> str:
    return hashlib.md5(prompt.encode()).hexdigest()
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### æ€§èƒ½ç›‘æ§é¢æ¿
```python
# monitoring.py
import psutil
import time

def monitor_system_resources():
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    return {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "gpu_temp": get_gpu_temperature(),  # éœ€è¦é¢å¤–å®ç°
        "model_load_time": measure_model_load_time()
    }

def get_model_performance_metrics():
    """è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    return {
        "tokens_per_second": calculate_tokens_per_second(),
        "average_response_time": get_average_response_time(),
        "error_rate": calculate_error_rate()
    }
```

### è‡ªåŠ¨è°ƒä¼˜å»ºè®®
- **å†…å­˜ä¸è¶³**: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ (Q4/Q5)
- **å“åº”æ…¢**: å‡å°‘å¹¶å‘æ•°æˆ–åˆ‡æ¢æ›´å°æ¨¡å‹
- **å‡†ç¡®æ€§å·®**: æé«˜ temperature æˆ–åˆ‡æ¢æ›´å¤§æ¨¡å‹

## ğŸ”® æœªæ¥æ‰©å±•æ–¹å‘

### 1. å¤šæ¨¡æ€é›†æˆ
- å›¾åƒç†è§£ï¼šLLaVAã€Qwen-VL
- è¯­éŸ³å¤„ç†ï¼šWhisper æœ¬åœ°éƒ¨ç½²
- è§†é¢‘åˆ†æï¼šVideo-ChatGPT

### 2. é›†ç¾¤åŒ–éƒ¨ç½²
```yaml
# kubernetes é›†ç¾¤é…ç½®ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: local-llm-cluster
spec:
  replicas: 3
  selector:
    matchLabels:
      app: local-llm
  template:
    metadata:
      labels:
        app: local-llm
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

### 3. æ™ºèƒ½è´Ÿè½½å‡è¡¡
```python
class LoadBalancer:
    def __init__(self):
        self.nodes = [
            {"host": "localhost:11434", "load": 0, "models": ["llama3.1:8b"]},
            {"host": "192.168.1.100:11434", "load": 0, "models": ["qwen2.5:7b"]}
        ]

    def get_best_node(self, model_required: str):
        available_nodes = [n for n in self.nodes if model_required in n["models"]]
        return min(available_nodes, key=lambda x: x["load"])
```

## ğŸ“‹ æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜è§£å†³

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Ollama å¯åŠ¨å¤±è´¥ | ç«¯å£è¢«å ç”¨ | `lsof -i :11434` æŸ¥æ‰¾å¹¶ç»ˆæ­¢è¿›ç¨‹ |
| æ¨¡å‹ä¸‹è½½ç¼“æ…¢ | ç½‘ç»œé—®é¢˜ | ä½¿ç”¨ä»£ç†æˆ–é•œåƒæº |
| å†…å­˜ä¸è¶³ | ç³»ç»Ÿå¡é¡¿ | ä½¿ç”¨é‡åŒ–æ¨¡å‹æˆ–å‡å°‘å¹¶å‘ |
| GPU æœªä½¿ç”¨ | CPU å ç”¨é«˜ | è®¾ç½® `OLLAMA_GPU_LAYERS=99` |

### æ—¥å¿—åˆ†æ
```bash
# Ollama æ—¥å¿—
tail -f ~/.ollama/logs/server.log

# ä»£ç†æœåŠ¡æ—¥å¿—
python local-llm-proxy.py 2>&1 | tee logs/proxy.log

# ç³»ç»Ÿèµ„æºç›‘æ§
watch -n 1 'ps aux | grep ollama; free -h'
```

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [Ollama å®˜æ–¹æ–‡æ¡£](https://ollama.com/docs)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Claude Tools æŒ‡å—](/claude-tools/overview)
- [Mac M2 ä¼˜åŒ–æŒ‡å—](/optimization/m2-performance)

---

*æœ¬æ¶æ„è®¾è®¡å……åˆ†åˆ©ç”¨äº† Mac M2 èŠ¯ç‰‡çš„ç¡¬ä»¶ä¼˜åŠ¿ï¼Œç»“åˆäº‘ç«¯å’Œæœ¬åœ°AIçš„æœ€ä½³å®è·µï¼Œä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæ—¢ç»æµåˆé«˜æ•ˆçš„AIå·¥å…·é“¾è§£å†³æ–¹æ¡ˆã€‚*