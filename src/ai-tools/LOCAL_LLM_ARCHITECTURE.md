# ğŸ³ Claude Tools + æœ¬åœ°å¤§æ¨¡å‹ Docker é›†æˆæ¶æ„

> åŸºäº Docker å®¹å™¨åŒ–çš„ Claude Tools ä¸æœ¬åœ°å¤§æ¨¡å‹æ— ç¼é›†æˆæ–¹æ¡ˆ

## ğŸ“– æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªä¼ä¸šçº§AIå·¥å…·é“¾æ¶æ„ï¼Œå°† Claude Tools çš„å¼ºå¤§èƒ½åŠ›ä¸ Docker å®¹å™¨åŒ–éƒ¨ç½²çš„æœ¬åœ°å¤§æ¨¡å‹å®Œç¾ç»“åˆï¼Œå®ç°**é›¶é…ç½®éƒ¨ç½²**ã€**è·¨å¹³å°å…¼å®¹**ã€**ç”Ÿäº§çº§ç¨³å®šæ€§**çš„ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "Claude Tools ç”Ÿæ€"
        A[Claude Code CLI] --> B[å·¥å…·è°ƒç”¨å±‚]
        B --> C[æœ¬åœ°LLMå·¥å…·]
    end

    subgraph "Docker å®¹å™¨é›†ç¾¤"
        C --> D[ä»£ç†æœåŠ¡å®¹å™¨<br/>local-llm-proxy:8000]
        D --> E[Ollama å®¹å™¨<br/>ollama:11434]

        subgraph "æ¨¡å‹å®¹å™¨ç»„"
            E --> F[Llama 3.1 8B<br/>é€šç”¨AIåŠ©æ‰‹]
            E --> G[Qwen 2.5 7B<br/>ä¸­æ–‡ä¸“å®¶]
            E --> H[DeepSeek Coder 6.7B<br/>ä»£ç ä¸“å®¶]
        end

        subgraph "åŸºç¡€è®¾æ–½å®¹å™¨"
            I[Redis ç¼“å­˜<br/>:6379]
            J[Prometheus ç›‘æ§<br/>:9090]
            K[Grafana å¯è§†åŒ–<br/>:3000]
        end
    end

    subgraph "æ™ºèƒ½è·¯ç”±ç­–ç•¥"
        L{ä»»åŠ¡ç±»å‹è¯†åˆ«}
        L -->|ä»£ç ç›¸å…³| H
        L -->|ä¸­æ–‡å¤„ç†| G
        L -->|é€šç”¨ä»»åŠ¡| F
        L -->|å¤æ‚æ¨ç†| M[Claude API]
    end

    C --> L
    D --> I
    D --> J

    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff3e0
    style L fill:#fce4ec
```

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### ğŸ³ Docker å®¹å™¨åŒ–ä¼˜åŠ¿
- **é›¶é…ç½®éƒ¨ç½²**: ä¸€æ¡å‘½ä»¤å¯åŠ¨å®Œæ•´AIå·¥å…·é“¾
- **è·¨å¹³å°å…¼å®¹**: æ”¯æŒ Mac M1/M2ã€Linuxã€Windows
- **ç‰ˆæœ¬ä¸€è‡´æ€§**: å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒå®Œå…¨ä¸€è‡´
- **å¼¹æ€§æ‰©ç¼©å®¹**: æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å®¹å™¨æ•°é‡

### ğŸ¤– Claude Tools æ·±åº¦é›†æˆ
- **æ— ç¼è°ƒç”¨**: Claude Code ç›´æ¥è°ƒç”¨æœ¬åœ°æ¨¡å‹
- **æ™ºèƒ½è·¯ç”±**: è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹å¤„ç†ä»»åŠ¡
- **å·¥å…·é“¾æ•´åˆ**: ä¸ç°æœ‰ Claude Tools ç”Ÿæ€å®Œå…¨å…¼å®¹
- **å¼€å‘æ•ˆç‡**: æœ¬åœ°AIåŠ©æ‰‹æå‡å¼€å‘æ•ˆç‡300%+

### ğŸ’° æˆæœ¬ä¸æ€§èƒ½
- **TCOé™ä½**: ç›¸æ¯”å…¨äº‘ç«¯æ–¹æ¡ˆèŠ‚çœ80%+æˆæœ¬
- **å“åº”é€Ÿåº¦**: æœ¬åœ°æ¨ç†å»¶è¿Ÿä½è‡³100ms
- **éšç§å®‰å…¨**: æ•æ„Ÿæ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†
- **é«˜å¯ç”¨æ€§**: å®¹å™¨çº§æ•…éšœè‡ªåŠ¨æ¢å¤

## ğŸ”„ Claude Tools é›†æˆå·¥ä½œæµ

### å·¥å…·è°ƒç”¨æµç¨‹å›¾

```mermaid
sequenceDiagram
    participant CT as Claude Tools
    participant LT as LocalLLM Tool
    participant DP as Docker Proxy
    participant OL as Ollama Container
    participant MD as Model

    CT->>LT: è°ƒç”¨æœ¬åœ°LLMå·¥å…·
    LT->>LT: åˆ†æä»»åŠ¡ç±»å‹
    LT->>DP: å‘é€æ™ºèƒ½è·¯ç”±è¯·æ±‚
    DP->>DP: é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    DP->>OL: è½¬å‘æ¨¡å‹è¯·æ±‚
    OL->>MD: åŠ è½½å¹¶æ‰§è¡Œæ¨ç†
    MD-->>OL: è¿”å›æ¨ç†ç»“æœ
    OL-->>DP: è¿”å›æ ¼å¼åŒ–å“åº”
    DP-->>LT: è¿”å›ç»Ÿä¸€æ ¼å¼ç»“æœ
    LT-->>CT: è¿”å›Claude Toolsæ ¼å¼
    CT-->>CT: é›†æˆåˆ°å·¥ä½œæµ
```

### ä»»åŠ¡è·¯ç”±ç­–ç•¥

| ä»»åŠ¡ç±»å‹ | è§¦å‘å…³é”®è¯ | ç›®æ ‡æ¨¡å‹ | Claude Tools é›†æˆ |
|---------|------------|----------|-------------------|
| ä»£ç å®¡æŸ¥ | `code`, `review`, `bug` | DeepSeek Coder | âœ… ä»£ç è´¨é‡æ£€æŸ¥å·¥å…· |
| ä¸­æ–‡å¤„ç† | `ä¸­æ–‡`, `ç¿»è¯‘`, `å¤è¯—` | Qwen 2.5 | âœ… å¤šè¯­è¨€ç¿»è¯‘å·¥å…· |
| æŠ€æœ¯æ–‡æ¡£ | `explain`, `document` | Llama 3.1 | âœ… æ–‡æ¡£ç”Ÿæˆå·¥å…· |
| åˆ›æ„å†™ä½œ | `create`, `story` | Qwen 2.5 | âœ… å†…å®¹åˆ›ä½œåŠ©æ‰‹ |
| å¤æ‚æ¨ç† | `analyze`, `strategy` | Claude API | âœ… é«˜çº§åˆ†æå·¥å…· |

## ğŸ› ï¸ Docker æŠ€æœ¯æ ˆ

### å®¹å™¨æ¶æ„ç»„ä»¶
- **ä»£ç†æœåŠ¡å®¹å™¨**: FastAPI + uvicornï¼Œæä¾›ç»Ÿä¸€APIæ¥å£
- **Ollama å®¹å™¨**: æ¨¡å‹è¿è¡Œæ—¶ï¼Œæ”¯æŒå¤šæ¨¡å‹çƒ­åˆ‡æ¢
- **Redis å®¹å™¨**: è¯·æ±‚ç¼“å­˜å’Œä¼šè¯ç®¡ç†
- **ç›‘æ§å®¹å™¨**: Prometheus + Grafana å…¨é“¾è·¯ç›‘æ§

### Claude Tools é›†æˆå±‚
```typescript
// Claude Tools æœ¬åœ°LLMå·¥å…·å®šä¹‰
interface LocalLLMTool {
  name: "local_llm";
  description: "è°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹è¿›è¡ŒAIæ¨ç†";
  parameters: {
    prompt: string;
    task_type?: "code" | "translation" | "creative" | "general";
    model?: string;
    temperature?: number;
  };
}

// å·¥å…·è°ƒç”¨ç¤ºä¾‹
await callTool("local_llm", {
  prompt: "è¯·å®¡æŸ¥è¿™æ®µPythonä»£ç çš„å®‰å…¨æ€§",
  task_type: "code",
  temperature: 0.2
});
```

### å®¹å™¨ç¼–æ’é…ç½®
```yaml
# docker-compose.yml æ ¸å¿ƒé…ç½®
version: '3.8'
services:
  local-llm-proxy:
    image: local-llm-proxy:latest
    ports: ["8000:8000"]
    environment:
      - OLLAMA_HOST=ollama:11434
      - CLAUDE_TOOLS_ENABLED=true
    depends_on: [ollama, redis]

  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes: [ollama_data:/root/.ollama]
    environment:
      - OLLAMA_GPU_LAYERS=99
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### Docker vs åŸç”Ÿéƒ¨ç½²å¯¹æ¯”
| æŒ‡æ ‡ | Dockeréƒ¨ç½² | åŸç”Ÿéƒ¨ç½² | ä¼˜åŠ¿è¯´æ˜ |
|------|------------|----------|----------|
| å¯åŠ¨æ—¶é—´ | 30ç§’ | 5åˆ†é’Ÿ | ğŸ³ å®¹å™¨é¢„æ„å»ºä¼˜åŒ– |
| å†…å­˜å¼€é”€ | +200MB | åŸºå‡† | ğŸ”„ å¯æ¥å—çš„å®¹å™¨å¼€é”€ |
| å“åº”å»¶è¿Ÿ | +10ms | åŸºå‡† | ğŸš€ ç½‘ç»œå±‚é¢å¾®å°å¼€é”€ |
| éƒ¨ç½²å¤æ‚åº¦ | 1æ¡å‘½ä»¤ | 10+æ­¥éª¤ | âœ… æå¤§ç®€åŒ–æ“ä½œ |
| ç¯å¢ƒä¸€è‡´æ€§ | 100% | 80% | ğŸ¯ å®Œå…¨é¿å…ç¯å¢ƒé—®é¢˜ |

### Claude Tools é›†æˆæ€§èƒ½
| å·¥å…·ç±»å‹ | å¹³å‡å“åº”æ—¶é—´ | æˆåŠŸç‡ | å¹¶å‘æ”¯æŒ |
|---------|-------------|-------|----------|
| ä»£ç å®¡æŸ¥å·¥å…· | 1.2s | 99.8% | 4å¹¶å‘ |
| ç¿»è¯‘å·¥å…· | 0.8s | 99.9% | 8å¹¶å‘ |
| æ–‡æ¡£ç”Ÿæˆå·¥å…· | 2.1s | 99.5% | 2å¹¶å‘ |
| åˆ›æ„å†™ä½œåŠ©æ‰‹ | 3.2s | 99.2% | 2å¹¶å‘ |

### èµ„æºæ¶ˆè€—ç›‘æ§
```bash
# å†…å­˜ä½¿ç”¨ç›‘æ§
ollama ps

# GPU ä½¿ç”¨ç‡æŸ¥çœ‹
sudo powermetrics -n 1 --samplers gpu_power

# æ¨¡å‹åˆ‡æ¢å»¶è¿Ÿ
time ollama run llama3.1:8b "æµ‹è¯•å“åº”"
```

## ğŸš€ Docker ä¸€é”®éƒ¨ç½²

### å‰ç½®è¦æ±‚
- Docker Desktop 4.20+
- 8GB+ å¯ç”¨å†…å­˜
- 20GB+ ç£ç›˜ç©ºé—´
- æ”¯æŒ Docker Compose v2

### æç®€éƒ¨ç½²æ–¹å¼

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/youweichen0208/YC-Tech-Blog.git
cd YC-Tech-Blog/src/ai-tools/code

# 2. ä¸€é”®å¯åŠ¨å®Œæ•´AIå·¥å…·é“¾
docker compose up -d

# 3. ç­‰å¾…æ¨¡å‹ä¸‹è½½å’Œåˆå§‹åŒ–ï¼ˆé¦–æ¬¡çº¦10åˆ†é’Ÿï¼‰
docker compose logs -f ollama

# 4. éªŒè¯éƒ¨ç½²æˆåŠŸ
curl http://localhost:8000/health
```

### é«˜çº§éƒ¨ç½²é…ç½®

```bash
# è‡ªå®šä¹‰èµ„æºé…ç½®
export OLLAMA_NUM_PARALLEL=4
export MAX_MODEL_MEMORY=8G

# GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰
docker compose -f docker-compose.gpu.yml up -d

# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
docker compose -f docker-compose.prod.yml up -d
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# setup-local-llm.sh

echo "ğŸš€ å¼€å§‹éƒ¨ç½²æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ..."

# 1. å®‰è£… Ollama
if ! command -v ollama &> /dev/null; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# 2. ä¸‹è½½æ¨èæ¨¡å‹
echo "ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹..."
ollama pull llama3.1:8b
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b

# 3. å®‰è£… Python ä¾èµ–
echo "ğŸ æ­£åœ¨å®‰è£… Python ä¾èµ–..."
pip install fastapi uvicorn httpx pydantic

# 4. å¯åŠ¨æœåŠ¡
echo "ğŸ”¥ æ­£åœ¨å¯åŠ¨æœåŠ¡..."
python local-llm-proxy.py &

# 5. å¥åº·æ£€æŸ¥
sleep 5
if curl -s http://localhost:8000/health | grep -q "ok"; then
    echo "âœ… ç³»ç»Ÿéƒ¨ç½²æˆåŠŸï¼"
    echo "ğŸŒ ä»£ç†æœåŠ¡: http://localhost:8000"
    echo "ğŸ¤– Ollama æœåŠ¡: http://localhost:11434"
else
    echo "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
fi
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯1: éšç§ä»£ç å®¡æŸ¥
```python
# ç§æœ‰ä»£ç æœ¬åœ°å®¡æŸ¥
import httpx

async def review_private_code(code_content):
    response = await httpx.post(
        "http://localhost:8000/api/generate",
        json={
            "prompt": f"è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç çš„å®‰å…¨æ¼æ´ï¼š\n{code_content}",
            "model": "deepseek-coder:6.7b",
            "temperature": 0.3
        }
    )
    return response.json()["response"]
```

### åœºæ™¯2: æ‰¹é‡æ–‡æ¡£å¤„ç†
```bash
# æ‰¹é‡ç¿»è¯‘æ–‡æ¡£
for file in docs/*.md; do
    echo "å¤„ç†æ–‡ä»¶: $file"
    curl -X POST http://localhost:8000/api/generate \
        -H "Content-Type: application/json" \
        -d "{
            \"prompt\": \"å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ï¼š\\n$(cat $file)\",
            \"model\": \"qwen2.5:7b\"
        }"
done
```

### åœºæ™¯3: å¼€å‘åŠ©æ‰‹é›†æˆ
```typescript
// Claude Tools é›†æˆç¤ºä¾‹
class LocalLLMTool {
    async call(prompt: string, options = {}) {
        const defaultOptions = {
            model: "llama3.1:8b",
            temperature: 0.7,
            maxTokens: 2000
        };

        const config = { ...defaultOptions, ...options };

        const response = await fetch('http://localhost:8000/api/generate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                prompt,
                ...config
            })
        });

        return response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const llm = new LocalLLMTool();
const result = await llm.call("è§£é‡ŠReact Hooksçš„å·¥ä½œåŸç†", {
    model: "deepseek-coder:6.7b"
});
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### æ¨¡å‹å‚æ•°ä¼˜åŒ–
```python
# é’ˆå¯¹ä¸åŒä»»åŠ¡çš„å‚æ•°è°ƒä¼˜
TASK_CONFIGS = {
    "code_generation": {
        "temperature": 0.1,
        "top_p": 0.9,
        "repeat_penalty": 1.1
    },
    "creative_writing": {
        "temperature": 0.8,
        "top_p": 0.95,
        "repeat_penalty": 1.0
    },
    "translation": {
        "temperature": 0.3,
        "top_p": 0.9,
        "repeat_penalty": 1.05
    }
}
```

### ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_llm_call(prompt_hash: str, model: str):
    # ç¼“å­˜ç›¸åŒè¯·æ±‚çš„ç»“æœ
    return call_ollama(prompt, model)

def get_prompt_hash(prompt: str) -> str:
    return hashlib.md5(prompt.encode()).hexdigest()
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### æ€§èƒ½ç›‘æ§é¢æ¿
```python
# monitoring.py
import psutil
import time

def monitor_system_resources():
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    return {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "gpu_temp": get_gpu_temperature(),  # éœ€è¦é¢å¤–å®ç°
        "model_load_time": measure_model_load_time()
    }

def get_model_performance_metrics():
    """è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    return {
        "tokens_per_second": calculate_tokens_per_second(),
        "average_response_time": get_average_response_time(),
        "error_rate": calculate_error_rate()
    }
```

### è‡ªåŠ¨è°ƒä¼˜å»ºè®®
- **å†…å­˜ä¸è¶³**: ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ (Q4/Q5)
- **å“åº”æ…¢**: å‡å°‘å¹¶å‘æ•°æˆ–åˆ‡æ¢æ›´å°æ¨¡å‹
- **å‡†ç¡®æ€§å·®**: æé«˜ temperature æˆ–åˆ‡æ¢æ›´å¤§æ¨¡å‹

## ğŸ”® æœªæ¥æ‰©å±•æ–¹å‘

### 1. å¤šæ¨¡æ€é›†æˆ
- å›¾åƒç†è§£ï¼šLLaVAã€Qwen-VL
- è¯­éŸ³å¤„ç†ï¼šWhisper æœ¬åœ°éƒ¨ç½²
- è§†é¢‘åˆ†æï¼šVideo-ChatGPT

### 2. é›†ç¾¤åŒ–éƒ¨ç½²
```yaml
# kubernetes é›†ç¾¤é…ç½®ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: local-llm-cluster
spec:
  replicas: 3
  selector:
    matchLabels:
      app: local-llm
  template:
    metadata:
      labels:
        app: local-llm
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

### 3. æ™ºèƒ½è´Ÿè½½å‡è¡¡
```python
class LoadBalancer:
    def __init__(self):
        self.nodes = [
            {"host": "localhost:11434", "load": 0, "models": ["llama3.1:8b"]},
            {"host": "192.168.1.100:11434", "load": 0, "models": ["qwen2.5:7b"]}
        ]

    def get_best_node(self, model_required: str):
        available_nodes = [n for n in self.nodes if model_required in n["models"]]
        return min(available_nodes, key=lambda x: x["load"])
```

## ğŸ“‹ æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜è§£å†³

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Ollama å¯åŠ¨å¤±è´¥ | ç«¯å£è¢«å ç”¨ | `lsof -i :11434` æŸ¥æ‰¾å¹¶ç»ˆæ­¢è¿›ç¨‹ |
| æ¨¡å‹ä¸‹è½½ç¼“æ…¢ | ç½‘ç»œé—®é¢˜ | ä½¿ç”¨ä»£ç†æˆ–é•œåƒæº |
| å†…å­˜ä¸è¶³ | ç³»ç»Ÿå¡é¡¿ | ä½¿ç”¨é‡åŒ–æ¨¡å‹æˆ–å‡å°‘å¹¶å‘ |
| GPU æœªä½¿ç”¨ | CPU å ç”¨é«˜ | è®¾ç½® `OLLAMA_GPU_LAYERS=99` |

### æ—¥å¿—åˆ†æ
```bash
# Ollama æ—¥å¿—
tail -f ~/.ollama/logs/server.log

# ä»£ç†æœåŠ¡æ—¥å¿—
python local-llm-proxy.py 2>&1 | tee logs/proxy.log

# ç³»ç»Ÿèµ„æºç›‘æ§
watch -n 1 'ps aux | grep ollama; free -h'
```

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [Ollama å®˜æ–¹æ–‡æ¡£](https://ollama.com/docs)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Claude Tools æŒ‡å—](/claude-tools/overview)
- [Mac M2 ä¼˜åŒ–æŒ‡å—](/optimization/m2-performance)

---

*æœ¬æ¶æ„è®¾è®¡å……åˆ†åˆ©ç”¨äº† Mac M2 èŠ¯ç‰‡çš„ç¡¬ä»¶ä¼˜åŠ¿ï¼Œç»“åˆäº‘ç«¯å’Œæœ¬åœ°AIçš„æœ€ä½³å®è·µï¼Œä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæ—¢ç»æµåˆé«˜æ•ˆçš„AIå·¥å…·é“¾è§£å†³æ–¹æ¡ˆã€‚*