---
title: Claude CLI + LiteLLM + æœ¬åœ°å¤§æ¨¡å‹é›†æˆæŒ‡å—
icon: rocket
order: 3
---

# Claude CLI + LiteLLM + æœ¬åœ°å¤§æ¨¡å‹é›†æˆæŒ‡å—

## ğŸ“– æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ **LiteLLM** ä½œä¸ºç»Ÿä¸€ä»£ç†å±‚ï¼Œè®© **Claude CLI** èƒ½å¤Ÿè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„å¼€æºå¤§æ¨¡å‹ï¼Œå®ç°ï¼š

- âœ… ç»Ÿä¸€ API æ¥å£ï¼ˆOpenAI/Anthropic æ ¼å¼ï¼‰
- âœ… å¤šæ¨¡å‹è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
- âœ… æˆæœ¬è·Ÿè¸ªå’Œç›‘æ§
- âœ… æµå¼å“åº”æ”¯æŒ
- âœ… Claude CLI æ— ç¼é›†æˆ

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Claude CLI                             â”‚
â”‚  (é€šè¿‡ ANTHROPIC_BASE_URL æŒ‡å‘ LiteLLM ä»£ç†)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP API
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LiteLLM ä»£ç†å±‚                           â”‚
â”‚  â€¢ ç»Ÿä¸€ API æ ¼å¼è½¬æ¢                                         â”‚
â”‚  â€¢ æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡                                        â”‚
â”‚  â€¢ æˆæœ¬è·Ÿè¸ªå’Œç›‘æ§                                            â”‚
â”‚  â€¢ æµå¼å“åº”å¤„ç†                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚              â”‚
       â†“              â†“              â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama  â”‚  â”‚  vLLM    â”‚  â”‚  Claude  â”‚  â”‚ OpenAI   â”‚
â”‚  (æœ¬åœ°)  â”‚  â”‚  (æœ¬åœ°)  â”‚  â”‚  (å®˜æ–¹)  â”‚  â”‚  (å®˜æ–¹)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’» ç®—åŠ›å¹³å°é€‰æ‹©

åœ¨éƒ¨ç½²æœ¬åœ°å¤§æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„ç®—åŠ›å¹³å°ã€‚ä»¥ä¸‹æ˜¯ä¸»æµç®—åŠ›å¹³å°å¯¹æ¯”ï¼š

### å¹³å°å…¼å®¹æ€§å¯¹æ¯”

| ç®—åŠ›å¹³å° | Ollama | vLLM | æ¨ç†æ€§èƒ½ | æ¨èåœºæ™¯ |
|---------|--------|------|---------|---------|
| ğŸ® **è‹±ä¼Ÿè¾¾ GPU** | âœ… å®Œç¾æ”¯æŒ | âœ… å®Œç¾æ”¯æŒ | â­â­â­â­â­ | ğŸ† é¦–é€‰æ–¹æ¡ˆ |
| ğŸ‡¨ğŸ‡³ **åä¸ºå‡è…¾ NPU** | âŒ ä¸æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | â­â­â­â­ | å›½äº§åŒ–éœ€æ±‚ |
| ğŸ **Apple Silicon (M1/M2/M3)** | âœ… åŸç”Ÿæ”¯æŒ | âŒ ä¸æ”¯æŒ | â­â­â­ | ä¸ªäººå¼€å‘ |
| ğŸ”´ **AMD GPU (ROCm)** | âš ï¸ éœ€ç¼–è¯‘ | âš ï¸ éœ€ç¼–è¯‘ | â­â­â­ | AMD è®¾å¤‡ |
| âš¡ **Intel GPU (oneAPI)** | âŒ å®éªŒæ€§ | âŒ å®éªŒæ€§ | â­â­ | Intel Arc |
| ğŸ–¥ï¸ **CPU (çº¯CPU)** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | â­ | æµ‹è¯•ç¯å¢ƒ |

### 1. è‹±ä¼Ÿè¾¾ GPU éƒ¨ç½² ğŸ® (æ¨è)

**ç¡¬ä»¶è¦æ±‚ï¼š**
- GPU æ˜¾å­˜ï¼šâ‰¥ 16GB (æ¨è 24GB+)
- æ¨èå‹å·ï¼šRTX 4090 / A100 / A6000 / V100

**äº‘æœåŠ¡å™¨æ¨èï¼š**
- é˜¿é‡Œäº‘ GPU äº‘æœåŠ¡å™¨ (ecs.gn7i-c8g1.2xlarge)
- è…¾è®¯äº‘ GPU äº‘æœåŠ¡å™¨ (GN10Xp)
- AWS EC2 (p3.2xlarge / g5.xlarge)
- Google Cloud Compute Engine (n1-standard-8 + T4)

**éƒ¨ç½²æ­¥éª¤ï¼š**

```bash
# 1. éªŒè¯ NVIDIA é©±åŠ¨
nvidia-smi

# 2. å®‰è£… CUDA Toolkit (å¦‚æœªå®‰è£…)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"
sudo apt-get update
sudo apt-get install cuda-12-2

# 3. å®‰è£… Ollama (è‡ªåŠ¨ä½¿ç”¨ CUDA)
curl -fsSL https://ollama.com/install.sh | sh

# 4. ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:7b       # éœ€è¦ ~5GB æ˜¾å­˜
ollama pull deepseek-coder:6.7b  # éœ€è¦ ~4GB æ˜¾å­˜
ollama pull llama3.1:8b      # éœ€è¦ ~5GB æ˜¾å­˜

# 5. éªŒè¯ GPU åŠ é€Ÿ
ollama run qwen2.5:7b "æµ‹è¯•GPUåŠ é€Ÿ"
# ä½¿ç”¨ nvidia-smi è§‚å¯Ÿ GPU ä½¿ç”¨ç‡
watch -n 1 nvidia-smi
```

**æ€§èƒ½åŸºå‡†ï¼ˆè‹±ä¼Ÿè¾¾ RTX 4090ï¼‰ï¼š**
- Qwen2.5 7B: ~80-100 tokens/s
- DeepSeek-Coder 6.7B: ~90-110 tokens/s
- Llama 3.1 8B: ~75-95 tokens/s

### 2. åä¸ºå‡è…¾ NPU éƒ¨ç½² ğŸ‡¨ğŸ‡³

**ç¡¬ä»¶è¦æ±‚ï¼š**
- å‡è…¾ 310P / 910B
- é©±åŠ¨ç‰ˆæœ¬ï¼šCANN 7.0+

**äº‘æœåŠ¡å™¨æ¨èï¼š**
- åä¸ºäº‘è€€äº‘æœåŠ¡å™¨ L å®ä¾‹ (ai1s.xlarge)
- åä¸ºäº‘ ECS é€šç”¨è®¡ç®—å¢å¼ºå‹ (c7.xlarge.2)

**éƒ¨ç½²æ­¥éª¤ï¼ˆvLLMï¼‰ï¼š**

```bash
# 1. å®‰è£… CANN é©±åŠ¨å’Œå›ºä»¶
# å‚è€ƒåä¸ºå®˜æ–¹æ–‡æ¡£ï¼šhttps://www.hiascend.com/document

# 2. éªŒè¯ NPU çŠ¶æ€
npu-smi info

# 3. å®‰è£…æ”¯æŒå‡è…¾çš„ vLLM (éœ€è¦ç‰¹æ®Šç¼–è¯‘ç‰ˆæœ¬)
pip install vllm-ascend  # åä¸ºæä¾›çš„é€‚é…ç‰ˆæœ¬

# 4. å¯åŠ¨ vLLM æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --device ascend \
  --tensor-parallel-size 1 \
  --port 8001

# 5. é…ç½® LiteLLM è¿æ¥ vLLM
# è§åç»­é…ç½®ç« èŠ‚
```

**âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**
- Ollama ç›®å‰ä¸æ”¯æŒåä¸ºå‡è…¾
- vLLM éœ€è¦ä½¿ç”¨åä¸ºå®˜æ–¹é€‚é…ç‰ˆæœ¬
- éƒ¨åˆ†æ¨¡å‹å¯èƒ½éœ€è¦è½¬æ¢æ ¼å¼ï¼ˆONNX â†’ OMï¼‰
- æ€§èƒ½çº¦ä¸ºåŒçº§åˆ«è‹±ä¼Ÿè¾¾ GPU çš„ 70-80%

### 3. Apple Silicon éƒ¨ç½² ğŸ (ä¸ªäººå¼€å‘)

**ç¡¬ä»¶è¦æ±‚ï¼š**
- M1/M2/M3 ç³»åˆ—èŠ¯ç‰‡
- ç»Ÿä¸€å†…å­˜ï¼šâ‰¥ 16GB (æ¨è 32GB+)

**éƒ¨ç½²æ­¥éª¤ï¼š**

```bash
# 1. å®‰è£… Ollama (åŸç”Ÿæ”¯æŒ Metal)
curl -fsSL https://ollama.com/install.sh | sh

# 2. ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b

# 3. éªŒè¯ Metal åŠ é€Ÿ
ollama run qwen2.5:7b "æµ‹è¯•MetalåŠ é€Ÿ"
# ä½¿ç”¨ Activity Monitor è§‚å¯Ÿ GPU ä½¿ç”¨ç‡
```

**æ€§èƒ½åŸºå‡†ï¼ˆM2 Max 32GBï¼‰ï¼š**
- Qwen2.5 7B: ~25-35 tokens/s
- DeepSeek-Coder 6.7B: ~30-40 tokens/s

### 4. AMD GPU éƒ¨ç½² ğŸ”´

**ç¡¬ä»¶è¦æ±‚ï¼š**
- AMD Radeon RX 7900 XTX / MI250X
- ROCm 5.7+

**éƒ¨ç½²æ­¥éª¤ï¼š**

```bash
# 1. å®‰è£… ROCm
sudo apt-get install rocm-hip-sdk

# 2. ä»æºç ç¼–è¯‘ Ollama (ROCm æ”¯æŒ)
git clone https://github.com/ollama/ollama.git
cd ollama
USE_ROCM=1 make

# 3. å¯åŠ¨æœåŠ¡
./ollama serve
```

### 5. çº¯ CPU éƒ¨ç½² ğŸ–¥ï¸ (æµ‹è¯•ç¯å¢ƒ)

**é€‚ç”¨åœºæ™¯ï¼š**
- å¼€å‘æµ‹è¯•
- ä½é¢‘ä½¿ç”¨
- æ—  GPU ç¯å¢ƒ

```bash
# å®‰è£… Ollama (è‡ªåŠ¨ä½¿ç”¨ CPU)
curl -fsSL https://ollama.com/install.sh | sh

# ä¸‹è½½è¾ƒå°çš„æ¨¡å‹
ollama pull qwen2.5:1.5b  # CPU å‹å¥½
ollama pull llama3.2:3b   # CPU å‹å¥½
```

**æ€§èƒ½åŸºå‡†ï¼ˆ32æ ¸ CPUï¼‰ï¼š**
- Qwen2.5 7B: ~5-8 tokens/s âš ï¸ è¾ƒæ…¢

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… LiteLLM

```bash
# ä½¿ç”¨ pip å®‰è£…
pip install litellm[proxy]

# æˆ–ä½¿ç”¨ Docker
docker pull ghcr.io/berriai/litellm:main-latest
```

### 2. éƒ¨ç½²æœ¬åœ°æ¨¡å‹æ¨ç†æœåŠ¡

æ ¹æ®ä½ çš„ç®—åŠ›å¹³å°ï¼Œé€‰æ‹©å¯¹åº”çš„éƒ¨ç½²æ–¹å¼ï¼š

#### æ–¹æ¡ˆ Aï¼šOllama (æ¨èç”¨äºè‹±ä¼Ÿè¾¾ GPU / Mac M ç³»åˆ—)

```bash
# å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b
ollama pull llama3.1:8b

# éªŒè¯æœåŠ¡
curl http://localhost:11434/api/tags
```

#### æ–¹æ¡ˆ Bï¼švLLM (æ¨èç”¨äºåä¸ºå‡è…¾ / é«˜æ€§èƒ½åœºæ™¯)

```bash
# å®‰è£… vLLM
pip install vllm

# å¯åŠ¨æ¨ç†æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --port 8001

# éªŒè¯æœåŠ¡
curl http://localhost:8001/v1/models
```

### 3. é…ç½® LiteLLM

åˆ›å»º `litellm_config.yaml`ï¼Œæ ¹æ®ä½ çš„æ¨ç†æœåŠ¡é…ç½®ï¼š

#### é…ç½® Aï¼šå¯¹æ¥ Ollama

```yaml
model_list:
  # Anthropic æ ¼å¼çš„æ¨¡å‹æ˜ å°„åˆ° Ollama
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://localhost:11434

  - model_name: claude-3-opus-20240229
    litellm_params:
      model: ollama/deepseek-coder:6.7b
      api_base: http://localhost:11434

  # ä¹Ÿå¯ä»¥åŒæ—¶æ”¯æŒå®˜æ–¹ Claude APIï¼ˆå›é€€æ–¹æ¡ˆï¼‰
  - model_name: claude-3-5-sonnet-20241022-official
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}
```

#### é…ç½® Bï¼šå¯¹æ¥ vLLM (åä¸ºå‡è…¾ç­‰)

```yaml
model_list:
  # æ˜ å°„åˆ° vLLM æœåŠ¡
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: openai/Qwen/Qwen2.5-7B-Instruct
      api_base: http://localhost:8001/v1

  - model_name: claude-3-opus-20240229
    litellm_params:
      model: openai/deepseek-ai/deepseek-coder-6.7b-instruct
      api_base: http://localhost:8001/v1

  # å›é€€åˆ°å®˜æ–¹ API
  - model_name: claude-3-5-sonnet-20241022-official
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}
```

#### é€šç”¨é…ç½®ï¼ˆä¸¤ç§æ–¹æ¡ˆå…±ç”¨ï¼‰

```yaml
# é€šç”¨é…ç½®
litellm_settings:
  # æµå¼å“åº”æ”¯æŒ
  stream: true

  # æˆæœ¬è·Ÿè¸ª
  success_callback: ["langfuse"]

  # é‡è¯•ç­–ç•¥
  num_retries: 3
  request_timeout: 600

  # å¹¶å‘é™åˆ¶
  max_parallel_requests: 10

  # ç¼“å­˜é…ç½®ï¼ˆå¯é€‰ï¼‰
  cache: true
  cache_params:
    type: redis
    host: localhost
    port: 6379

# è·¯ç”±ç­–ç•¥
router_settings:
  routing_strategy: least-busy
  model_group_alias:
    gpt-4: ollama/qwen2.5:7b
    gpt-3.5-turbo: ollama/llama3.1:8b

# ç›‘æ§é…ç½®
general_settings:
  master_key: sk-1234  # ç”¨äºè®¤è¯çš„ä¸»å¯†é’¥
  database_url: sqlite:///litellm.db  # å­˜å‚¨è¯·æ±‚æ—¥å¿—
```

### 4. å¯åŠ¨ LiteLLM ä»£ç†

```bash
# æ–¹å¼1ï¼šç›´æ¥å¯åŠ¨
litellm --config litellm_config.yaml --port 8000

# æ–¹å¼2ï¼šç”Ÿäº§æ¨¡å¼ï¼ˆä½¿ç”¨ gunicornï¼‰
litellm --config litellm_config.yaml --port 8000 --num_workers 4

# æ–¹å¼3ï¼šDocker å¯åŠ¨
docker run -d \
  --name litellm-proxy \
  -p 8000:8000 \
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \
  -e ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY} \
  ghcr.io/berriai/litellm:main-latest \
  --config /app/config.yaml --port 8000
```

### 5. é…ç½® Claude CLI

ç¼–è¾‘ `~/.claude/config.json` æˆ–é¡¹ç›®çš„ `.claude/config.json`ï¼š

```json
{
  "apiKey": "sk-1234",  // ä¸ litellm_config.yaml ä¸­çš„ master_key ä¸€è‡´
  "baseURL": "http://localhost:8000/v1"
}
```

æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```bash
export ANTHROPIC_BASE_URL="http://localhost:8000/v1"
export ANTHROPIC_AUTH_TOKEN="sk-1234"
```

### 6. æµ‹è¯•é›†æˆ

```bash
# æµ‹è¯• LiteLLM ä»£ç†æ˜¯å¦æ­£å¸¸
curl http://localhost:8000/health

# æµ‹è¯•æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models \
  -H "Authorization: Bearer sk-1234"

# ä½¿ç”¨ Claude CLI æµ‹è¯•
claude "ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯ LiteLLM"
```

## âš™ï¸ é«˜çº§é…ç½®

### æ™ºèƒ½è·¯ç”±ç­–ç•¥

LiteLLM æ”¯æŒå¤šç§è·¯ç”±ç­–ç•¥ï¼š

```yaml
router_settings:
  # ç­–ç•¥1ï¼šæœ€å°‘å¿™ç¢Œï¼ˆæ¨èï¼‰
  routing_strategy: least-busy

  # ç­–ç•¥2ï¼šè½®è¯¢
  # routing_strategy: simple-shuffle

  # ç­–ç•¥3ï¼šæˆæœ¬ä¼˜å…ˆ
  # routing_strategy: cost-based

  # ç­–ç•¥4ï¼šå»¶è¿Ÿä¼˜å…ˆ
  # routing_strategy: latency-based

  # å›é€€é…ç½®
  allowed_fails: 3
  cooldown_time: 30  # å¤±è´¥åå†·å´æ—¶é—´ï¼ˆç§’ï¼‰
```

### æ¨¡å‹ç»„å’Œè´Ÿè½½å‡è¡¡

```yaml
model_list:
  # åŒä¸€ä¸ªæ¨¡å‹åå¯ä»¥æ˜ å°„åˆ°å¤šä¸ªåç«¯
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://localhost:11434

  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/deepseek-coder:6.7b
      api_base: http://localhost:11434

  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}
```

### æˆæœ¬è·Ÿè¸ªé›†æˆ

```yaml
litellm_settings:
  # ä½¿ç”¨ Langfuse è·Ÿè¸ª
  success_callback: ["langfuse"]
  langfuse_public_key: ${LANGFUSE_PUBLIC_KEY}
  langfuse_secret_key: ${LANGFUSE_SECRET_KEY}
  langfuse_host: https://cloud.langfuse.com
```

### Redis ç¼“å­˜åŠ é€Ÿ

```yaml
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: localhost
    port: 6379
    password: ${REDIS_PASSWORD}
    ttl: 3600  # ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰

    # ç¼“å­˜é”®ç­–ç•¥
    supported_call_types: ["completion", "acompletion", "embedding"]
```

## ğŸ³ Docker Compose éƒ¨ç½²

åˆ›å»º `docker-compose.yml`ï¼š

```yaml
version: '3.8'

services:
  # LiteLLM ä»£ç†
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "8000:8000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    command: --config /app/config.yaml --port 8000 --num_workers 4
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
  redis:
    image: redis:7-alpine
    container_name: litellm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  # Prometheus ç›‘æ§ï¼ˆå¯é€‰ï¼‰
  prometheus:
    image: prom/prometheus:latest
    container_name: litellm-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    restart: unless-stopped

  # Grafana å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
  grafana:
    image: grafana/grafana:latest
    container_name: litellm-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f litellm

# åœæ­¢æœåŠ¡
docker-compose down
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### å†…ç½®ç›‘æ§ç«¯ç‚¹

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models -H "Authorization: Bearer sk-1234"

# ç»Ÿè®¡ä¿¡æ¯
curl http://localhost:8000/stats -H "Authorization: Bearer sk-1234"

# è¯·æ±‚æ—¥å¿—
curl http://localhost:8000/logs -H "Authorization: Bearer sk-1234"
```

### Prometheus æŒ‡æ ‡

LiteLLM è‡ªåŠ¨æš´éœ² Prometheus æŒ‡æ ‡ï¼š

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'litellm'
    static_configs:
      - targets: ['litellm:8000']
    metrics_path: '/metrics'
```

å¯ç”¨æŒ‡æ ‡ï¼š
- `litellm_requests_total` - æ€»è¯·æ±‚æ•°
- `litellm_requests_duration_seconds` - è¯·æ±‚å»¶è¿Ÿ
- `litellm_requests_errors_total` - é”™è¯¯æ•°
- `litellm_model_requests_total` - æ¯ä¸ªæ¨¡å‹çš„è¯·æ±‚æ•°
- `litellm_cost_total` - æ€»æˆæœ¬

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šLiteLLM æ— æ³•è¿æ¥åˆ° Ollama

**ç—‡çŠ¶ï¼š**
```
Error: Connection refused to http://localhost:11434
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
ps aux | grep ollama

# é‡å¯ Ollama
ollama serve

# æµ‹è¯•è¿æ¥
curl http://localhost:11434/api/tags
```

### é—®é¢˜2ï¼šClaude CLI æŠ¥é”™ 401 Unauthorized

**ç—‡çŠ¶ï¼š**
```
Error: Unauthorized (401)
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ç¡®ä¿ API Key åŒ¹é…
# 1. æ£€æŸ¥ litellm_config.yaml ä¸­çš„ master_key
# 2. æ£€æŸ¥ Claude CLI é…ç½®ä¸­çš„ apiKey

# æ–¹å¼1ï¼šæ›´æ–° config.json
cat ~/.claude/config.json

# æ–¹å¼2ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
export ANTHROPIC_AUTH_TOKEN="sk-1234"
```

### é—®é¢˜3ï¼šå“åº”é€Ÿåº¦æ…¢

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**

1. **æ¨¡å‹åŠ è½½æ—¶é—´é•¿**
   ```bash
   # é¢„çƒ­æ¨¡å‹
   curl -X POST http://localhost:11434/api/generate \
     -d '{"model": "qwen2.5:7b", "prompt": "hello", "stream": false}'
   ```

2. **å¯ç”¨ Redis ç¼“å­˜**
   ```yaml
   litellm_settings:
     cache: true
     cache_params:
       type: redis
       host: localhost
       port: 6379
   ```

3. **å¢åŠ å¹¶å‘å¤„ç†**
   ```bash
   # å¯åŠ¨æ—¶å¢åŠ  worker æ•°é‡
   litellm --config litellm_config.yaml --num_workers 4
   ```

### é—®é¢˜4ï¼šæµå¼å“åº”ä¸å·¥ä½œ

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# ç¡®ä¿é…ç½®ä¸­å¯ç”¨äº†æµå¼å“åº”
litellm_settings:
  stream: true

# æµ‹è¯•æµå¼ API
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": true
  }'
```

### é—®é¢˜5ï¼šDocker å®¹å™¨å†…æ— æ³•è®¿é—®å®¿ä¸»æœºæœåŠ¡

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# ä½¿ç”¨ host.docker.internalï¼ˆMac/Windowsï¼‰
model_list:
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://host.docker.internal:11434

# æˆ–ä½¿ç”¨ç½‘ç»œæ¨¡å¼ï¼ˆLinuxï¼‰
# docker-compose.yml
services:
  litellm:
    network_mode: host
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```yaml
# æŒ‰ä»»åŠ¡ç±»å‹è·¯ç”±åˆ°ä¸åŒæ¨¡å‹
model_list:
  # ä»£ç ç”Ÿæˆä»»åŠ¡
  - model_name: claude-3-5-sonnet-code
    litellm_params:
      model: ollama/deepseek-coder:6.7b
      api_base: http://localhost:11434

  # é€šç”¨å¯¹è¯ä»»åŠ¡
  - model_name: claude-3-5-sonnet-chat
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://localhost:11434

  # é•¿æ–‡æœ¬å¤„ç†
  - model_name: claude-3-opus-long
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://localhost:11434
```

åœ¨ Claude CLI ä¸­æŒ‡å®šæ¨¡å‹ï¼š
```bash
claude --model claude-3-5-sonnet-code "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"
```

### 2. æˆæœ¬ä¼˜åŒ–

```yaml
# ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¤±è´¥æ—¶å›é€€åˆ°äº‘ç«¯
model_list:
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://localhost:11434

  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}

router_settings:
  routing_strategy: cost-based
  fallbacks: true
  allowed_fails: 2
```

### 3. æ€§èƒ½ä¼˜åŒ–

```yaml
litellm_settings:
  # å¯ç”¨ç¼“å­˜
  cache: true
  cache_params:
    type: redis
    ttl: 3600

  # æ‰¹å¤„ç†
  batch_size: 5

  # è¶…æ—¶æ§åˆ¶
  request_timeout: 300

  # è¿æ¥æ± 
  max_parallel_requests: 20
```

### 4. å®‰å…¨é…ç½®

```yaml
general_settings:
  # API å¯†é’¥ç®¡ç†
  master_key: ${LITELLM_MASTER_KEY}  # ä»ç¯å¢ƒå˜é‡è¯»å–

  # ç”¨æˆ·è®¤è¯
  user_api_key_auth: true

  # é€Ÿç‡é™åˆ¶
  rpm: 100  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
  tpm: 100000  # æ¯åˆ†é’Ÿ token æ•°

  # IP ç™½åå•
  allowed_ips: ["127.0.0.1", "192.168.1.0/24"]
```

## ğŸ“š ç›¸å…³èµ„æº

- ğŸ“– [LiteLLM å®˜æ–¹æ–‡æ¡£](https://docs.litellm.ai/)
- ğŸ™ [LiteLLM GitHub](https://github.com/BerriAI/litellm)
- ğŸ¦™ [Ollama æ–‡æ¡£](https://ollama.com/docs)
- ğŸ¤– [Claude CLI æ–‡æ¡£](https://docs.claude.com/claude-code)

## ğŸ‰ æ€»ç»“

é€šè¿‡ LiteLLM é›†æˆæœ¬åœ°å¤§æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ï¼š

âœ… **é™ä½æˆæœ¬** - æœ¬åœ°æ¨ç†èŠ‚çœ 99%+ API è´¹ç”¨
âœ… **ä¿æŠ¤éšç§** - æ•æ„Ÿæ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†
âœ… **æå‡çµæ´»æ€§** - ç»Ÿä¸€æ¥å£è°ƒç”¨å¤šç§æ¨¡å‹
âœ… **ä¼˜åŒ–æ€§èƒ½** - æ™ºèƒ½è·¯ç”±å’Œç¼“å­˜åŠ é€Ÿ
âœ… **æ— ç¼é›†æˆ** - Claude CLI é›¶æ”¹åŠ¨ä½¿ç”¨

ç°åœ¨å¼€å§‹ä½¿ç”¨ LiteLLMï¼Œè®© Claude CLI æ›´å¼ºå¤§ï¼ğŸš€
