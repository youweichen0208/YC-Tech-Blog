# ğŸš€ æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿå¿«é€Ÿå¼€å§‹æŒ‡å—

è¿™æ˜¯ä¸€ä¸ª5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹æŒ‡å—ï¼Œå¸®åŠ©ä½ åœ¨Mac M2ä¸Šå¿«é€Ÿéƒ¨ç½²æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

- Mac M2/M3 èŠ¯ç‰‡ï¼ˆæ¨èï¼‰
- macOS 12.0 æˆ–æ›´é«˜ç‰ˆæœ¬
- 8GB+ å†…å­˜ï¼ˆæ¨è16GB+ï¼‰
- 20GB+ å¯ç”¨å­˜å‚¨ç©ºé—´

## âš¡ ä¸€é”®éƒ¨ç½²

### æ–¹å¼1: è„šæœ¬è‡ªåŠ¨éƒ¨ç½²ï¼ˆæ¨èï¼‰

```bash
# 1. ä¸‹è½½éƒ¨ç½²è„šæœ¬
curl -O https://raw.githubusercontent.com/youweichen0208/YC-Tech-Blog/master/src/ai-tools/code/setup-local-llm.sh

# 2. ç»™æ‰§è¡Œæƒé™
chmod +x setup-local-llm.sh

# 3. ä¸€é”®éƒ¨ç½²
./setup-local-llm.sh install
```

### æ–¹å¼2: æ‰‹åŠ¨éƒ¨ç½²

```bash
# 1. å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. å¯åŠ¨ Ollama
ollama serve &

# 3. ä¸‹è½½æ¨¡å‹
ollama pull llama3.1:8b
ollama pull qwen2.5:7b

# 4. å®‰è£… Python ä¾èµ–
pip install fastapi uvicorn httpx pydantic psutil

# 5. ä¸‹è½½ä»£ç†æœåŠ¡
curl -O https://raw.githubusercontent.com/youweichen0208/YC-Tech-Blog/master/src/ai-tools/code/local_llm_proxy.py

# 6. å¯åŠ¨ä»£ç†æœåŠ¡
python local_llm_proxy.py
```

## ğŸ§ª å¿«é€Ÿæµ‹è¯•

éƒ¨ç½²å®Œæˆåï¼Œå¯ä»¥è¿›è¡Œä»¥ä¸‹æµ‹è¯•ï¼š

### 1. å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8000/health
```

æœŸæœ›è¾“å‡ºï¼š
```json
{
  "status": "healthy",
  "timestamp": "2024-10-28T10:30:00",
  "ollama_connected": true,
  "total_requests": 0
}
```

### 2. æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
    "model": "qwen2.5:7b",
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

### 3. ä»£ç å®¡æŸ¥æµ‹è¯•
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "è¯·å®¡æŸ¥ä»¥ä¸‹Pythonä»£ç ï¼š\ndef hello():\n    print(\"Hello World\")",
    "model": "deepseek-coder:6.7b",
    "temperature": 0.3
  }'
```

## ğŸ”§ åŸºç¡€ä½¿ç”¨

### Pythoné›†æˆç¤ºä¾‹

```python
import httpx
import asyncio

async def call_local_llm(prompt, model="llama3.1:8b"):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/api/generate",
            json={
                "prompt": prompt,
                "model": model,
                "temperature": 0.7,
                "max_tokens": 500
            }
        )
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    result = await call_local_llm("å†™ä¸€ä¸ªPythonçš„å¿«é€Ÿæ’åºç®—æ³•")
    print(result["response"])

asyncio.run(main())
```

### curlå‘½ä»¤ç¤ºä¾‹

```bash
# ä¸­æ–‡å¯¹è¯
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "model": "qwen2.5:7b"}'

# ä»£ç ç”Ÿæˆ
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "å†™ä¸€ä¸ªJavaScriptçš„å†’æ³¡æ’åº", "model": "deepseek-coder:6.7b"}'

# æ–‡æœ¬ç¿»è¯‘
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "ç¿»è¯‘æˆè‹±æ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½", "model": "qwen2.5:7b"}'
```

## ğŸ“Š ç›‘æ§é¢æ¿

è®¿é—®ä»¥ä¸‹åœ°å€æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€ï¼š

- **APIæ–‡æ¡£**: http://localhost:8000/docs
- **ç³»ç»ŸçŠ¶æ€**: http://localhost:8000/api/status
- **æ¨¡å‹åˆ—è¡¨**: http://localhost:8000/api/models

## ğŸ›ï¸ å¸¸ç”¨ç®¡ç†å‘½ä»¤

```bash
# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
./setup-local-llm.sh monitor

# å¯åŠ¨æœåŠ¡
./setup-local-llm.sh start

# åœæ­¢æœåŠ¡
./setup-local-llm.sh stop

# è¿è¡Œæµ‹è¯•
./setup-local-llm.sh test

# æ¸…ç†ç³»ç»Ÿ
./setup-local-llm.sh clean
```

## ğŸ”„ åˆ‡æ¢æ¨¡å‹

ç³»ç»Ÿæ”¯æŒæ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼š

```bash
# ä½¿ç”¨ Llama 3.1ï¼ˆé€šç”¨ä»»åŠ¡ï¼‰
curl -X POST http://localhost:8000/api/generate \
  -d '{"prompt": "ä½ å¥½", "model": "llama3.1:8b"}'

# ä½¿ç”¨ Qwen 2.5ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
curl -X POST http://localhost:8000/api/generate \
  -d '{"prompt": "å†™ä¸€é¦–å¤è¯—", "model": "qwen2.5:7b"}'

# ä½¿ç”¨ DeepSeek Coderï¼ˆä»£ç ä¸“ç”¨ï¼‰
curl -X POST http://localhost:8000/api/generate \
  -d '{"prompt": "è§£é‡Šè¿™æ®µä»£ç ", "model": "deepseek-coder:6.7b"}'
```

## âš ï¸ å¸¸è§é—®é¢˜

### 1. ç«¯å£è¢«å ç”¨
```bash
# æŸ¥çœ‹ç«¯å£å ç”¨
lsof -i :11434
lsof -i :8000

# æ›´æ”¹ç«¯å£
python local_llm_proxy.py --port 8001
```

### 2. å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
ollama pull llama3.1:8b-q4_0

# å‡å°‘å¹¶å‘æ•°
export OLLAMA_NUM_PARALLEL=1
```

### 3. å“åº”æ…¢
```bash
# æ£€æŸ¥GPUä½¿ç”¨
system_profiler SPDisplaysDataType

# ä¼˜åŒ–è®¾ç½®
export OLLAMA_GPU_LAYERS=99
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜

### å†…å­˜ä¼˜åŒ–
```bash
# 8GB å†…å­˜é…ç½®
export OLLAMA_MAX_LOADED_MODELS=1
ollama pull llama3.1:8b-q4_0

# 16GB å†…å­˜é…ç½®
export OLLAMA_MAX_LOADED_MODELS=2
ollama pull llama3.1:8b
ollama pull qwen2.5:7b

# 24GB+ å†…å­˜é…ç½®
export OLLAMA_MAX_LOADED_MODELS=3
ollama pull llama3.1:8b
ollama pull qwen2.5:14b
ollama pull deepseek-coder:6.7b
```

### å¹¶å‘ä¼˜åŒ–
```bash
# æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
export OLLAMA_NUM_PARALLEL=2  # M2 æ¨èå€¼
export OLLAMA_NUM_PARALLEL=4  # M2 Pro/Max æ¨èå€¼
```

## ğŸ”— ä¸‹ä¸€æ­¥

1. **é›†æˆåˆ°ç°æœ‰é¡¹ç›®**: æŸ¥çœ‹ [Claude Toolsé›†æˆæŒ‡å—](./claude_tools_integration.py)
2. **Dockeréƒ¨ç½²**: ä½¿ç”¨ [docker-compose.yml](./docker-compose.yml) è¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²
3. **ç”Ÿäº§ç¯å¢ƒ**: å‚è€ƒ [å®Œæ•´æ¶æ„æ–‡æ¡£](./LOCAL_LLM_ARCHITECTURE.md)
4. **ç›‘æ§è¿ç»´**: é…ç½® Prometheus + Grafana ç›‘æ§

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **ä»»åŠ¡åˆ†é…**ï¼šå¤æ‚æ¨ç†ä½¿ç”¨Claude APIï¼Œç®€å•ä»»åŠ¡ä½¿ç”¨æœ¬åœ°æ¨¡å‹
2. **æ¨¡å‹é€‰æ‹©**ï¼šä»£ç ç›¸å…³ç”¨DeepSeekï¼Œä¸­æ–‡ä»»åŠ¡ç”¨Qwenï¼Œå…¶ä»–ç”¨Llama
3. **å‚æ•°è°ƒä¼˜**ï¼šä»£ç ç”Ÿæˆç”¨ä½temperature(0.1-0.3)ï¼Œåˆ›æ„å†™ä½œç”¨é«˜temperature(0.7-0.9)
4. **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†å¤§é‡é‡å¤æ€§ä»»åŠ¡ï¼ŒèŠ‚çœAPIè´¹ç”¨

---

ğŸ‰ **æ­å–œï¼ä½ å·²ç»æˆåŠŸéƒ¨ç½²äº†æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿï¼**

ç°åœ¨å¯ä»¥äº«å—é«˜æ€§èƒ½ã€ä½æˆæœ¬ã€éšç§å®‰å…¨çš„AIå·¥å…·é“¾äº†ã€‚