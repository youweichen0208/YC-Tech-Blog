"""
Claude Tools æœ¬åœ°å¤§æ¨¡å‹é›†æˆå·¥å…·
ä¸º Claude Code æä¾›æœ¬åœ°å¤§æ¨¡å‹è°ƒç”¨èƒ½åŠ›
"""

import asyncio
import json
from typing import Dict, Any, Optional, List
import httpx
from dataclasses import dataclass
from enum import Enum


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CODE_REVIEW = "code_review"
    TRANSLATION = "translation"
    SUMMARIZATION = "summarization"
    QUESTION_ANSWER = "question_answer"
    CREATIVE_WRITING = "creative_writing"
    DATA_EXTRACTION = "data_extraction"
    GENERAL = "general"


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    LLAMA = "llama3.1:8b"
    QWEN = "qwen2.5:7b"
    DEEPSEEK_CODER = "deepseek-coder:6.7b"


@dataclass
class LocalLLMConfig:
    """æœ¬åœ°å¤§æ¨¡å‹é…ç½®"""
    base_url: str = "http://localhost:8000"
    timeout: float = 60.0
    max_retries: int = 3
    default_model: str = ModelType.LLAMA.value
    temperature: float = 0.7
    max_tokens: int = 2000


class LocalLLMClient:
    """æœ¬åœ°å¤§æ¨¡å‹å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[LocalLLMConfig] = None):
        self.config = config or LocalLLMConfig()
        self.client = httpx.AsyncClient(timeout=self.config.timeout)

        # ä»»åŠ¡ç±»å‹åˆ°æ¨¡å‹çš„æ˜ å°„
        self.task_model_mapping = {
            TaskType.CODE_REVIEW: ModelType.DEEPSEEK_CODER.value,
            TaskType.TRANSLATION: ModelType.QWEN.value,
            TaskType.SUMMARIZATION: ModelType.LLAMA.value,
            TaskType.QUESTION_ANSWER: ModelType.QWEN.value,
            TaskType.CREATIVE_WRITING: ModelType.QWEN.value,
            TaskType.DATA_EXTRACTION: ModelType.LLAMA.value,
            TaskType.GENERAL: ModelType.LLAMA.value,
        }

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()

    async def generate(
        self,
        prompt: str,
        task_type: TaskType = TaskType.GENERAL,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: è¾“å…¥æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            model: æŒ‡å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        # é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
        selected_model = model or self.task_model_mapping[task_type]

        # æ„å»ºè¯·æ±‚å‚æ•°
        request_data = {
            "prompt": prompt,
            "model": selected_model,
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            **kwargs
        }

        # å‘é€è¯·æ±‚
        for attempt in range(self.config.max_retries):
            try:
                response = await self.client.post(
                    f"{self.config.base_url}/api/generate",
                    json=request_data
                )
                response.raise_for_status()
                return response.json()

            except httpx.HTTPError as e:
                if attempt == self.config.max_retries - 1:
                    raise Exception(f"è¯·æ±‚å¤±è´¥: {str(e)}")
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    async def chat(
        self,
        messages: List[Dict[str, str]],
        task_type: TaskType = TaskType.GENERAL,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤šè½®å¯¹è¯

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            task_type: ä»»åŠ¡ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å¯¹è¯ç»“æœ
        """
        # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå•ä¸ªprompt
        prompt = self._format_messages(messages)
        return await self.generate(prompt, task_type, **kwargs)

    def _format_messages(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯æ¶ˆæ¯"""
        formatted = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                formatted.append(f"System: {content}")
            elif role == "user":
                formatted.append(f"User: {content}")
            elif role == "assistant":
                formatted.append(f"Assistant: {content}")

        return "\n".join(formatted) + "\nAssistant:"

    async def get_models(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = await self.client.get(f"{self.config.base_url}/api/models")
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            raise Exception(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

    async def get_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            response = await self.client.get(f"{self.config.base_url}/api/status")
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            raise Exception(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            response = await self.client.get(f"{self.config.base_url}/health")
            return response.status_code == 200
        except:
            return False


class ClaudeToolsIntegration:
    """Claude Tools é›†æˆç±»"""

    def __init__(self, llm_client: LocalLLMClient):
        self.llm_client = llm_client

    async def code_review_tool(
        self,
        code: str,
        language: str = "python",
        focus_areas: Optional[List[str]] = None
    ) -> str:
        """
        ä»£ç å®¡æŸ¥å·¥å…·

        Args:
            code: è¦å®¡æŸ¥çš„ä»£ç 
            language: ç¼–ç¨‹è¯­è¨€
            focus_areas: é‡ç‚¹å…³æ³¨é¢†åŸŸ

        Returns:
            å®¡æŸ¥ç»“æœ
        """
        focus_text = ""
        if focus_areas:
            focus_text = f"é‡ç‚¹å…³æ³¨: {', '.join(focus_areas)}\n"

        prompt = f"""è¯·å¯¹ä»¥ä¸‹{language}ä»£ç è¿›è¡Œè¯¦ç»†å®¡æŸ¥ï¼š

{focus_text}
ä»£ç ï¼š
```{language}
{code}
```

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»£ç å®‰å…¨æ€§
2. æ€§èƒ½ä¼˜åŒ–å»ºè®®
3. ä»£ç è§„èŒƒæ€§
4. æ½œåœ¨bug
5. å¯ç»´æŠ¤æ€§

è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚"""

        result = await self.llm_client.generate(
            prompt=prompt,
            task_type=TaskType.CODE_REVIEW,
            temperature=0.3
        )

        return result.get("response", "")

    async def translation_tool(
        self,
        text: str,
        source_lang: str = "auto",
        target_lang: str = "english"
    ) -> str:
        """
        ç¿»è¯‘å·¥å…·

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            source_lang: æºè¯­è¨€
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            ç¿»è¯‘ç»“æœ
        """
        prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_lang}ï¼š

åŸæ–‡ï¼š
{text}

è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„è¯­ä¹‰å’Œè¯­è°ƒ
2. ä½¿ç”¨åœ°é“çš„è¡¨è¾¾
3. å¦‚æœæœ‰ä¸“ä¸šæœ¯è¯­ï¼Œè¯·ä¿æŒå‡†ç¡®æ€§
4. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Š

ç¿»è¯‘ï¼š"""

        result = await self.llm_client.generate(
            prompt=prompt,
            task_type=TaskType.TRANSLATION,
            temperature=0.3
        )

        return result.get("response", "")

    async def summarization_tool(
        self,
        text: str,
        summary_type: str = "brief",
        max_length: int = 200
    ) -> str:
        """
        æ–‡æœ¬æ‘˜è¦å·¥å…·

        Args:
            text: è¦æ‘˜è¦çš„æ–‡æœ¬
            summary_type: æ‘˜è¦ç±»å‹ï¼ˆbrief, detailed, bullet_pointsï¼‰
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            æ‘˜è¦ç»“æœ
        """
        type_instructions = {
            "brief": "æä¾›ç®€æ´çš„æ‘˜è¦",
            "detailed": "æä¾›è¯¦ç»†çš„æ‘˜è¦",
            "bullet_points": "ä½¿ç”¨è¦ç‚¹å½¢å¼æ€»ç»“"
        }

        instruction = type_instructions.get(summary_type, "æä¾›æ‘˜è¦")

        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼š

æ–‡æœ¬ï¼š
{text}

è¦æ±‚ï¼š
1. {instruction}
2. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
3. ä¿ç•™å…³é”®ä¿¡æ¯
4. ä½¿ç”¨æ¸…æ™°çš„è¯­è¨€

æ‘˜è¦ï¼š"""

        result = await self.llm_client.generate(
            prompt=prompt,
            task_type=TaskType.SUMMARIZATION,
            temperature=0.5,
            max_tokens=max_length * 2
        )

        return result.get("response", "")

    async def qa_tool(
        self,
        question: str,
        context: Optional[str] = None,
        language: str = "chinese"
    ) -> str:
        """
        é—®ç­”å·¥å…·

        Args:
            question: é—®é¢˜
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            language: å›ç­”è¯­è¨€

        Returns:
            ç­”æ¡ˆ
        """
        context_text = f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context}" if context else ""

        prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä½¿ç”¨{language}å›ç­”ï¼š

é—®é¢˜ï¼š{question}{context_text}

è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†
2. å¦‚æœä¸ç¡®å®šï¼Œè¯·è¯´æ˜
3. æä¾›å®ç”¨çš„å»ºè®®æˆ–ä¿¡æ¯
4. ä½¿ç”¨æ¸…æ™°çš„è¯­è¨€ç»„ç»‡

å›ç­”ï¼š"""

        result = await self.llm_client.generate(
            prompt=prompt,
            task_type=TaskType.QUESTION_ANSWER,
            temperature=0.6
        )

        return result.get("response", "")

    async def data_extraction_tool(
        self,
        text: str,
        extraction_fields: List[str],
        output_format: str = "json"
    ) -> str:
        """
        æ•°æ®æå–å·¥å…·

        Args:
            text: æºæ–‡æœ¬
            extraction_fields: è¦æå–çš„å­—æ®µ
            output_format: è¾“å‡ºæ ¼å¼

        Returns:
            æå–ç»“æœ
        """
        fields_text = "ã€".join(extraction_fields)

        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æŒ‡å®šä¿¡æ¯ï¼š

æ–‡æœ¬ï¼š
{text}

éœ€è¦æå–çš„å­—æ®µï¼š{fields_text}

è¦æ±‚ï¼š
1. ä»¥{output_format}æ ¼å¼è¾“å‡º
2. å¦‚æœæŸä¸ªå­—æ®µæ²¡æœ‰æ‰¾åˆ°ï¼Œæ ‡è®°ä¸ºnull
3. ä¿æŒæ•°æ®çš„å‡†ç¡®æ€§
4. åªè¿”å›æå–ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Š

æå–ç»“æœï¼š"""

        result = await self.llm_client.generate(
            prompt=prompt,
            task_type=TaskType.DATA_EXTRACTION,
            temperature=0.2
        )

        return result.get("response", "")


# ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å…·å‡½æ•°
async def create_local_llm_tool(config: Optional[LocalLLMConfig] = None):
    """åˆ›å»ºæœ¬åœ°å¤§æ¨¡å‹å·¥å…·å®ä¾‹"""
    client = LocalLLMClient(config)
    return ClaudeToolsIntegration(client)


# ä¾¿æ·å‡½æ•°
async def local_code_review(code: str, language: str = "python") -> str:
    """ä¾¿æ·çš„ä»£ç å®¡æŸ¥å‡½æ•°"""
    async with LocalLLMClient() as client:
        integration = ClaudeToolsIntegration(client)
        return await integration.code_review_tool(code, language)


async def local_translate(text: str, target_lang: str = "english") -> str:
    """ä¾¿æ·çš„ç¿»è¯‘å‡½æ•°"""
    async with LocalLLMClient() as client:
        integration = ClaudeToolsIntegration(client)
        return await integration.translation_tool(text, target_lang=target_lang)


async def local_summarize(text: str, max_length: int = 200) -> str:
    """ä¾¿æ·çš„æ‘˜è¦å‡½æ•°"""
    async with LocalLLMClient() as client:
        integration = ClaudeToolsIntegration(client)
        return await integration.summarization_tool(text, max_length=max_length)


async def local_qa(question: str, context: Optional[str] = None) -> str:
    """ä¾¿æ·çš„é—®ç­”å‡½æ•°"""
    async with LocalLLMClient() as client:
        integration = ClaudeToolsIntegration(client)
        return await integration.qa_tool(question, context)


# æµ‹è¯•å’Œç¤ºä¾‹ä»£ç 
async def test_integration():
    """æµ‹è¯•é›†æˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æœ¬åœ°å¤§æ¨¡å‹é›†æˆ...")

    config = LocalLLMConfig(base_url="http://localhost:8000")

    async with LocalLLMClient(config) as client:
        # å¥åº·æ£€æŸ¥
        if not await client.health_check():
            print("âŒ æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœ¬åœ°å¤§æ¨¡å‹æœåŠ¡")
            return

        print("âœ… æœåŠ¡è¿æ¥æ­£å¸¸")

        integration = ClaudeToolsIntegration(client)

        # æµ‹è¯•ä»£ç å®¡æŸ¥
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""

        print("\nğŸ“ æµ‹è¯•ä»£ç å®¡æŸ¥...")
        review_result = await integration.code_review_tool(test_code, "python")
        print(f"å®¡æŸ¥ç»“æœï¼š{review_result[:200]}...")

        # æµ‹è¯•ç¿»è¯‘
        print("\nğŸŒ æµ‹è¯•ç¿»è¯‘åŠŸèƒ½...")
        translation_result = await integration.translation_tool(
            "Hello, how are you today?",
            target_lang="chinese"
        )
        print(f"ç¿»è¯‘ç»“æœï¼š{translation_result}")

        # æµ‹è¯•é—®ç­”
        print("\nâ“ æµ‹è¯•é—®ç­”åŠŸèƒ½...")
        qa_result = await integration.qa_tool("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
        print(f"é—®ç­”ç»“æœï¼š{qa_result[:200]}...")

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_integration())