"""
æœ¬åœ°å¤§æ¨¡å‹ä»£ç†æœåŠ¡
æ”¯æŒå¤šæ¨¡å‹è·¯ç”±å’Œæ™ºèƒ½è´Ÿè½½å‡è¡¡
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import httpx
import uvicorn
import asyncio
import time
import logging
import psutil
import hashlib
from functools import lru_cache
from datetime import datetime
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="æœ¬åœ°å¤§æ¨¡å‹ä»£ç†æœåŠ¡",
    description="é«˜æ€§èƒ½æœ¬åœ°å¤§æ¨¡å‹APIä»£ç†ï¼Œæ”¯æŒæ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®å¸¸é‡
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_TIMEOUT = 60.0
MAX_CACHE_SIZE = 1000

class LLMRequest(BaseModel):
    """LLMè¯·æ±‚æ¨¡å‹"""
    prompt: str = Field(..., description="è¾“å…¥æç¤ºè¯")
    model: str = Field(default="llama3.1:8b", description="æ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    max_tokens: int = Field(default=2000, ge=1, le=8192, description="æœ€å¤§ä»¤ç‰Œæ•°")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="Top-pé‡‡æ ·")
    repeat_penalty: float = Field(default=1.1, ge=0.0, le=2.0, description="é‡å¤æƒ©ç½š")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")

class LLMResponse(BaseModel):
    """LLMå“åº”æ¨¡å‹"""
    response: str = Field(..., description="æ¨¡å‹å›å¤")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    tokens_used: int = Field(..., description="ä½¿ç”¨çš„ä»¤ç‰Œæ•°")
    generation_time: float = Field(..., description="ç”Ÿæˆæ—¶é—´(ç§’)")
    cached: bool = Field(default=False, description="æ˜¯å¦æ¥è‡ªç¼“å­˜")

class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    size: str
    description: str
    capabilities: List[str]
    recommended_use: List[str]

class SystemStatus(BaseModel):
    """ç³»ç»ŸçŠ¶æ€"""
    cpu_percent: float
    memory_percent: float
    gpu_available: bool
    active_models: List[str]
    total_requests: int
    cache_hit_rate: float

# å…¨å±€çŠ¶æ€ç®¡ç†
class GlobalState:
    def __init__(self):
        self.request_count = 0
        self.cache_hits = 0
        self.active_models = set()
        self.model_stats = {}

global_state = GlobalState()

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "llama3.1:8b": {
        "description": "é€šç”¨è¯­è¨€æ¨¡å‹ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯å’Œæ–‡æœ¬ç”Ÿæˆ",
        "capabilities": ["æ–‡æœ¬ç”Ÿæˆ", "å¯¹è¯", "æ‘˜è¦", "ç¿»è¯‘"],
        "recommended_use": ["é€šç”¨ä»»åŠ¡", "å®¢æˆ·æœåŠ¡", "å†…å®¹åˆ›ä½œ"],
        "default_params": {"temperature": 0.7, "top_p": 0.9}
    },
    "qwen2.5:7b": {
        "description": "ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼Œä¼˜ç§€çš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›",
        "capabilities": ["ä¸­æ–‡å¯¹è¯", "ä¸­æ–‡åˆ›ä½œ", "ä¸­è‹±ç¿»è¯‘", "å¤è¯—è¯"],
        "recommended_use": ["ä¸­æ–‡å¤„ç†", "ç¿»è¯‘ä»»åŠ¡", "æ–‡åŒ–å†…å®¹"],
        "default_params": {"temperature": 0.6, "top_p": 0.9}
    },
    "deepseek-coder:6.7b": {
        "description": "ä»£ç ä¸“ç”¨æ¨¡å‹ï¼Œç²¾é€šå¤šç§ç¼–ç¨‹è¯­è¨€",
        "capabilities": ["ä»£ç ç”Ÿæˆ", "ä»£ç è§£é‡Š", "ä»£ç å®¡æŸ¥", "è°ƒè¯•"],
        "recommended_use": ["ç¼–ç¨‹åŠ©æ‰‹", "ä»£ç å®¡æŸ¥", "æŠ€æœ¯æ–‡æ¡£"],
        "default_params": {"temperature": 0.2, "top_p": 0.95}
    }
}

# ä»»åŠ¡ç±»å‹è¯†åˆ«
TASK_PATTERNS = {
    "code": [
        "ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "class", "def ", "function",
        "bug", "é”™è¯¯", "è°ƒè¯•", "review", "å®¡æŸ¥"
    ],
    "chinese": [
        "ä¸­æ–‡", "ç¿»è¯‘", "å¤è¯—", "æ–‡è¨€æ–‡", "æˆè¯­", "æ±‰è¯­"
    ],
    "creative": [
        "åˆ›ä½œ", "æ•…äº‹", "å°è¯´", "è¯—æ­Œ", "åˆ›æ„", "æƒ³è±¡"
    ]
}

def get_cache_key(prompt: str, model: str, params: dict) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    content = f"{prompt}:{model}:{json.dumps(params, sort_keys=True)}"
    return hashlib.md5(content.encode()).hexdigest()

@lru_cache(maxsize=MAX_CACHE_SIZE)
def cached_response(cache_key: str, response_data: str) -> str:
    """ç¼“å­˜å“åº”"""
    global_state.cache_hits += 1
    return response_data

def detect_task_type(prompt: str) -> str:
    """æ£€æµ‹ä»»åŠ¡ç±»å‹"""
    prompt_lower = prompt.lower()

    for task_type, patterns in TASK_PATTERNS.items():
        if any(pattern in prompt_lower for pattern in patterns):
            return task_type

    return "general"

def select_optimal_model(prompt: str, specified_model: Optional[str] = None) -> str:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©"""
    if specified_model and specified_model in MODEL_CONFIGS:
        return specified_model

    task_type = detect_task_type(prompt)

    model_mapping = {
        "code": "deepseek-coder:6.7b",
        "chinese": "qwen2.5:7b",
        "creative": "qwen2.5:7b",
        "general": "llama3.1:8b"
    }

    return model_mapping.get(task_type, "llama3.1:8b")

async def call_ollama(request: LLMRequest) -> Dict[str, Any]:
    """è°ƒç”¨Ollama API"""
    start_time = time.time()

    # æ„å»ºè¯·æ±‚å‚æ•°
    payload = {
        "model": request.model,
        "prompt": request.prompt,
        "stream": request.stream,
        "options": {
            "temperature": request.temperature,
            "num_predict": request.max_tokens,
            "top_p": request.top_p,
            "repeat_penalty": request.repeat_penalty
        }
    }

    try:
        async with httpx.AsyncClient(timeout=DEFAULT_TIMEOUT) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json=payload
            )
            response.raise_for_status()
            result = response.json()

            generation_time = time.time() - start_time

            return {
                "response": result.get("response", ""),
                "model": request.model,
                "tokens_used": result.get("eval_count", 0),
                "generation_time": generation_time,
                "cached": False
            }

    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="æ¨¡å‹å“åº”è¶…æ—¶")
    except httpx.HTTPError as e:
        raise HTTPException(status_code=500, detail=f"OllamaæœåŠ¡é”™è¯¯: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨é”™è¯¯: {str(e)}")

@app.post("/api/generate", response_model=LLMResponse)
async def generate_text(request: LLMRequest):
    """ç”Ÿæˆæ–‡æœ¬"""
    global_state.request_count += 1

    # æ™ºèƒ½æ¨¡å‹é€‰æ‹©
    optimal_model = select_optimal_model(request.prompt, request.model)
    request.model = optimal_model

    # æ£€æŸ¥ç¼“å­˜
    cache_key = get_cache_key(
        request.prompt,
        request.model,
        {
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
            "top_p": request.top_p,
            "repeat_penalty": request.repeat_penalty
        }
    )

    try:
        # è°ƒç”¨æ¨¡å‹
        result = await call_ollama(request)

        # æ›´æ–°çŠ¶æ€
        global_state.active_models.add(request.model)

        # è®°å½•ç»Ÿè®¡
        if request.model not in global_state.model_stats:
            global_state.model_stats[request.model] = {
                "requests": 0,
                "total_time": 0.0,
                "avg_time": 0.0
            }

        stats = global_state.model_stats[request.model]
        stats["requests"] += 1
        stats["total_time"] += result["generation_time"]
        stats["avg_time"] = stats["total_time"] / stats["requests"]

        logger.info(f"è¯·æ±‚å®Œæˆ - æ¨¡å‹: {request.model}, ç”¨æ—¶: {result['generation_time']:.2f}s")

        return LLMResponse(**result)

    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}")
        raise

@app.get("/api/models", response_model=List[ModelInfo])
async def list_models():
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            ollama_models = response.json().get("models", [])

            models = []
            for model in ollama_models:
                model_name = model.get("name", "")
                config = MODEL_CONFIGS.get(model_name, {})

                models.append(ModelInfo(
                    name=model_name,
                    size=model.get("size", "æœªçŸ¥"),
                    description=config.get("description", "è‡ªå®šä¹‰æ¨¡å‹"),
                    capabilities=config.get("capabilities", []),
                    recommended_use=config.get("recommended_use", [])
                ))

            return models

    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()

        cache_hit_rate = (
            global_state.cache_hits / global_state.request_count * 100
            if global_state.request_count > 0 else 0
        )

        return SystemStatus(
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            gpu_available=True,  # M2èŠ¯ç‰‡å§‹ç»ˆæ”¯æŒGPU
            active_models=list(global_state.active_models),
            total_requests=global_state.request_count,
            cache_hit_rate=cache_hit_rate
        )

    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")

@app.get("/api/stats")
async def get_model_stats():
    """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "model_stats": global_state.model_stats,
        "total_requests": global_state.request_count,
        "cache_hits": global_state.cache_hits,
        "active_models": list(global_state.active_models)
    }

@app.post("/api/chat")
async def chat_endpoint(request: LLMRequest):
    """èŠå¤©æ¥å£ï¼ˆæµå¼å“åº”ï¼‰"""
    request.stream = True

    try:
        async with httpx.AsyncClient(timeout=DEFAULT_TIMEOUT) as client:
            payload = {
                "model": request.model,
                "prompt": request.prompt,
                "stream": True,
                "options": {
                    "temperature": request.temperature,
                    "num_predict": request.max_tokens
                }
            }

            async with client.stream(
                "POST",
                f"{OLLAMA_BASE_URL}/api/generate",
                json=payload
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if data.get("response"):
                                yield f"data: {json.dumps({'content': data['response']})}\n\n"

                            if data.get("done"):
                                yield f"data: {json.dumps({'done': True})}\n\n"
                                break
                        except json.JSONDecodeError:
                            continue

    except Exception as e:
        logger.error(f"èŠå¤©æµå¼å“åº”å¤±è´¥: {str(e)}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥Ollamaè¿æ¥
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            response.raise_for_status()

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "ollama_connected": True,
            "total_requests": global_state.request_count
        }

    except Exception as e:
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "ollama_connected": False,
            "error": str(e)
        }

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ æœ¬åœ°å¤§æ¨¡å‹ä»£ç†æœåŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“¡ Ollamaåœ°å€: {OLLAMA_BASE_URL}")
    logger.info(f"ğŸ¯ æ”¯æŒæ¨¡å‹: {list(MODEL_CONFIGS.keys())}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info("ğŸ”Œ æœ¬åœ°å¤§æ¨¡å‹ä»£ç†æœåŠ¡å…³é—­")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å¯åŠ¨æœ¬åœ°å¤§æ¨¡å‹ä»£ç†æœåŠ¡")
    parser.add_argument("--host", default="0.0.0.0", help="ç»‘å®šä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç»‘å®šç«¯å£")
    parser.add_argument("--reload", action="store_true", help="å¼€å‘æ¨¡å¼çƒ­é‡è½½")
    parser.add_argument("--log-level", default="info", help="æ—¥å¿—çº§åˆ«")

    args = parser.parse_args()

    uvicorn.run(
        "local_llm_proxy:app",
        host=args.host,
        port=args.port,
        reload=args.reload,
        log_level=args.log_level
    )