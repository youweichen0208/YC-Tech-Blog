#!/bin/bash

# æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿä¸€é”®éƒ¨ç½²è„šæœ¬
# é€‚ç”¨äº Mac M2 èŠ¯ç‰‡

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # æ— é¢œè‰²

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
check_system() {
    log_step "æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ..."

    # æ£€æŸ¥æ˜¯å¦ä¸ºmacOS
    if [[ "$OSTYPE" != "darwin"* ]]; then
        log_error "æ­¤è„šæœ¬ä»…æ”¯æŒ macOS ç³»ç»Ÿ"
        exit 1
    fi

    # æ£€æŸ¥M2èŠ¯ç‰‡
    chip_info=$(sysctl -n machdep.cpu.brand_string)
    if [[ $chip_info == *"Apple M"* ]]; then
        log_info "æ£€æµ‹åˆ° Apple Silicon èŠ¯ç‰‡: $chip_info"
    else
        log_warn "æœªæ£€æµ‹åˆ° Apple Silicon èŠ¯ç‰‡ï¼Œæ€§èƒ½å¯èƒ½å—é™"
    fi

    # æ£€æŸ¥å†…å­˜
    memory_gb=$(( $(sysctl -n hw.memsize) / 1024 / 1024 / 1024 ))
    log_info "ç³»ç»Ÿå†…å­˜: ${memory_gb}GB"

    if [ $memory_gb -lt 8 ]; then
        log_error "ç³»ç»Ÿå†…å­˜ä¸è¶³8GBï¼Œæ— æ³•è¿è¡Œå¤§æ¨¡å‹"
        exit 1
    elif [ $memory_gb -lt 16 ]; then
        log_warn "å»ºè®®ä½¿ç”¨16GBæˆ–æ›´å¤šå†…å­˜ä»¥è·å¾—æ›´å¥½æ€§èƒ½"
    fi
}

# å®‰è£…ä¾èµ–
install_dependencies() {
    log_step "å®‰è£…ç³»ç»Ÿä¾èµ–..."

    # æ£€æŸ¥Homebrew
    if ! command -v brew &> /dev/null; then
        log_info "æ­£åœ¨å®‰è£… Homebrew..."
        /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    else
        log_info "Homebrew å·²å®‰è£…"
    fi

    # æ£€æŸ¥Python3
    if ! command -v python3 &> /dev/null; then
        log_info "æ­£åœ¨å®‰è£… Python3..."
        brew install python3
    else
        python_version=$(python3 --version)
        log_info "Python3 å·²å®‰è£…: $python_version"
    fi

    # æ£€æŸ¥pip
    if ! command -v pip3 &> /dev/null; then
        log_info "æ­£åœ¨å®‰è£… pip..."
        python3 -m ensurepip
    fi
}

# å®‰è£…Ollama
install_ollama() {
    log_step "å®‰è£… Ollama..."

    if ! command -v ollama &> /dev/null; then
        log_info "æ­£åœ¨ä¸‹è½½å¹¶å®‰è£… Ollama..."
        curl -fsSL https://ollama.com/install.sh | sh

        # ç­‰å¾…å®‰è£…å®Œæˆ
        sleep 3

        if command -v ollama &> /dev/null; then
            log_info "Ollama å®‰è£…æˆåŠŸ"
        else
            log_error "Ollama å®‰è£…å¤±è´¥"
            exit 1
        fi
    else
        log_info "Ollama å·²å®‰è£…"
    fi

    # å¯åŠ¨OllamaæœåŠ¡ï¼ˆå¦‚æœæœªè¿è¡Œï¼‰
    if ! pgrep -f "ollama serve" > /dev/null; then
        log_info "å¯åŠ¨ Ollama æœåŠ¡..."
        ollama serve &
        sleep 5
    else
        log_info "Ollama æœåŠ¡å·²è¿è¡Œ"
    fi
}

# ä¸‹è½½æ¨¡å‹
download_models() {
    log_step "ä¸‹è½½æ¨èæ¨¡å‹..."

    # æ¨¡å‹åˆ—è¡¨ï¼ˆæ ¹æ®å†…å­˜å¤§å°é€‰æ‹©ï¼‰
    memory_gb=$(( $(sysctl -n hw.memsize) / 1024 / 1024 / 1024 ))

    if [ $memory_gb -ge 24 ]; then
        models=("llama3.1:8b" "qwen2.5:14b" "deepseek-coder:6.7b")
    elif [ $memory_gb -ge 16 ]; then
        models=("llama3.1:8b" "qwen2.5:7b" "deepseek-coder:6.7b")
    else
        models=("llama3.1:8b" "qwen2.5:7b")
    fi

    for model in "${models[@]}"; do
        log_info "ä¸‹è½½æ¨¡å‹: $model"

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
        if ollama list | grep -q "$model"; then
            log_info "æ¨¡å‹ $model å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½"
        else
            ollama pull "$model"
            if [ $? -eq 0 ]; then
                log_info "æ¨¡å‹ $model ä¸‹è½½æˆåŠŸ"
            else
                log_error "æ¨¡å‹ $model ä¸‹è½½å¤±è´¥"
            fi
        fi
    done

    # åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹
    log_info "å·²å®‰è£…çš„æ¨¡å‹:"
    ollama list
}

# å®‰è£…Pythonä¾èµ–
install_python_deps() {
    log_step "å®‰è£… Python ä¾èµ–..."

    # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
    if [ ! -d "venv" ]; then
        log_info "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
        python3 -m venv venv
    fi

    # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
    source venv/bin/activate 2>/dev/null || true

    # å®‰è£…ä¾èµ–
    log_info "å®‰è£… Python åŒ…..."
    pip3 install fastapi uvicorn httpx pydantic psutil

    log_info "Python ä¾èµ–å®‰è£…å®Œæˆ"
}

# åˆ›å»ºé…ç½®æ–‡ä»¶
create_configs() {
    log_step "åˆ›å»ºé…ç½®æ–‡ä»¶..."

    # åˆ›å»ºç¯å¢ƒé…ç½®
    cat > .env << EOF
# Ollama é…ç½®
OLLAMA_HOST=http://localhost:11434
OLLAMA_GPU_LAYERS=99
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=3

# ä»£ç†æœåŠ¡é…ç½®
PROXY_HOST=0.0.0.0
PROXY_PORT=8000
LOG_LEVEL=info

# ç¼“å­˜é…ç½®
CACHE_SIZE=1000
ENABLE_CACHE=true

# æ€§èƒ½è°ƒä¼˜
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2000
REQUEST_TIMEOUT=60
EOF

    # åˆ›å»ºå¯åŠ¨è„šæœ¬
    cat > start.sh << 'EOF'
#!/bin/bash

# å¯åŠ¨æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ

# åŠ è½½ç¯å¢ƒå˜é‡
if [ -f .env ]; then
    export $(cat .env | grep -v '#' | xargs)
fi

# å¯åŠ¨Ollamaï¼ˆå¦‚æœæœªè¿è¡Œï¼‰
if ! pgrep -f "ollama serve" > /dev/null; then
    echo "å¯åŠ¨ Ollama æœåŠ¡..."
    ollama serve &
    sleep 5
fi

# å¯åŠ¨ä»£ç†æœåŠ¡
echo "å¯åŠ¨ä»£ç†æœåŠ¡..."
python3 local_llm_proxy.py --host $PROXY_HOST --port $PROXY_PORT --log-level $LOG_LEVEL
EOF

    chmod +x start.sh

    # åˆ›å»ºåœæ­¢è„šæœ¬
    cat > stop.sh << 'EOF'
#!/bin/bash

echo "åœæ­¢æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ..."

# åœæ­¢ä»£ç†æœåŠ¡
pkill -f "local_llm_proxy.py"

# åœæ­¢OllamaæœåŠ¡
pkill -f "ollama serve"

echo "æœåŠ¡å·²åœæ­¢"
EOF

    chmod +x stop.sh

    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    cat > test.sh << 'EOF'
#!/bin/bash

echo "æµ‹è¯•æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ..."

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 3

# æµ‹è¯•å¥åº·æ£€æŸ¥
echo "1. æµ‹è¯•å¥åº·æ£€æŸ¥..."
curl -s http://localhost:8000/health | python3 -m json.tool

echo -e "\n2. æµ‹è¯•æ¨¡å‹åˆ—è¡¨..."
curl -s http://localhost:8000/api/models | python3 -m json.tool

echo -e "\n3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ..."
curl -X POST http://localhost:8000/api/generate \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        "model": "qwen2.5:7b",
        "temperature": 0.7,
        "max_tokens": 100
    }' | python3 -m json.tool

echo -e "\n4. æµ‹è¯•ç³»ç»ŸçŠ¶æ€..."
curl -s http://localhost:8000/api/status | python3 -m json.tool

echo -e "\næµ‹è¯•å®Œæˆï¼"
EOF

    chmod +x test.sh

    log_info "é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ"
}

# ç³»ç»Ÿä¼˜åŒ–
optimize_system() {
    log_step "ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½..."

    # è®¾ç½®ç¯å¢ƒå˜é‡
    export OLLAMA_GPU_LAYERS=99
    export OLLAMA_NUM_PARALLEL=2
    export OLLAMA_MAX_LOADED_MODELS=3

    # åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬
    cat > monitor.sh << 'EOF'
#!/bin/bash

echo "æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿæ€§èƒ½ç›‘æ§"
echo "=========================="

while true; do
    clear
    echo "æ—¶é—´: $(date)"
    echo ""

    # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
    echo "ç³»ç»Ÿèµ„æº:"
    top -l 1 | grep "CPU usage"
    top -l 1 | grep "PhysMem"
    echo ""

    # Ollamaè¿›ç¨‹çŠ¶æ€
    echo "Ollama è¿›ç¨‹:"
    ps aux | grep ollama | grep -v grep || echo "Ollama æœªè¿è¡Œ"
    echo ""

    # ä»£ç†æœåŠ¡çŠ¶æ€
    echo "ä»£ç†æœåŠ¡çŠ¶æ€:"
    curl -s http://localhost:8000/health 2>/dev/null | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    print(f'çŠ¶æ€: {data[\"status\"]}')
    print(f'è¯·æ±‚æ€»æ•°: {data[\"total_requests\"]}')
except:
    print('ä»£ç†æœåŠ¡æœªå“åº”')
" || echo "ä»£ç†æœåŠ¡æœªè¿è¡Œ"

    echo ""
    echo "æŒ‰ Ctrl+C é€€å‡ºç›‘æ§"
    sleep 5
done
EOF

    chmod +x monitor.sh

    log_info "æ€§èƒ½ä¼˜åŒ–å®Œæˆ"
}

# å¯åŠ¨æœåŠ¡
start_services() {
    log_step "å¯åŠ¨æœåŠ¡..."

    # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
    if lsof -i :11434 >/dev/null 2>&1; then
        log_info "Ollama æœåŠ¡å·²åœ¨è¿è¡Œ"
    else
        log_info "å¯åŠ¨ Ollama æœåŠ¡..."
        ollama serve &
        sleep 5
    fi

    if lsof -i :8000 >/dev/null 2>&1; then
        log_warn "ç«¯å£ 8000 å·²è¢«å ç”¨ï¼Œè¯·æ£€æŸ¥æˆ–æ›´æ”¹ç«¯å£"
    else
        log_info "å¯åŠ¨ä»£ç†æœåŠ¡..."
        python3 local_llm_proxy.py &
        sleep 3
    fi
}

# è¿è¡Œæµ‹è¯•
run_tests() {
    log_step "è¿è¡Œç³»ç»Ÿæµ‹è¯•..."

    # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
    sleep 5

    # å¥åº·æ£€æŸ¥
    if curl -s http://localhost:8000/health | grep -q "healthy"; then
        log_info "âœ… å¥åº·æ£€æŸ¥é€šè¿‡"
    else
        log_error "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
        return 1
    fi

    # æ¨¡å‹æµ‹è¯•
    if curl -s http://localhost:8000/api/models | grep -q "llama"; then
        log_info "âœ… æ¨¡å‹åˆ—è¡¨è·å–æˆåŠŸ"
    else
        log_error "âŒ æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥"
        return 1
    fi

    # ç®€å•æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
    response=$(curl -s -X POST http://localhost:8000/api/generate \
        -H "Content-Type: application/json" \
        -d '{
            "prompt": "Hello",
            "model": "llama3.1:8b",
            "max_tokens": 10
        }')

    if echo "$response" | grep -q "response"; then
        log_info "âœ… æ–‡æœ¬ç”Ÿæˆæµ‹è¯•é€šè¿‡"
    else
        log_error "âŒ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥"
        return 1
    fi

    log_info "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼"
    return 0
}

# æ˜¾ç¤ºä½¿ç”¨ä¿¡æ¯
show_usage() {
    echo ""
    log_info "=========================================="
    log_info "ğŸ‰ æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿéƒ¨ç½²å®Œæˆï¼"
    log_info "=========================================="
    echo ""
    echo "ğŸ“¡ æœåŠ¡åœ°å€:"
    echo "   - ä»£ç†æœåŠ¡: http://localhost:8000"
    echo "   - OllamaæœåŠ¡: http://localhost:11434"
    echo "   - APIæ–‡æ¡£: http://localhost:8000/docs"
    echo ""
    echo "ğŸ› ï¸ ç®¡ç†å‘½ä»¤:"
    echo "   - å¯åŠ¨æœåŠ¡: ./start.sh"
    echo "   - åœæ­¢æœåŠ¡: ./stop.sh"
    echo "   - è¿è¡Œæµ‹è¯•: ./test.sh"
    echo "   - æ€§èƒ½ç›‘æ§: ./monitor.sh"
    echo ""
    echo "ğŸ“š å¿«é€Ÿæµ‹è¯•:"
    echo "   curl -X POST http://localhost:8000/api/generate \\"
    echo "     -H 'Content-Type: application/json' \\"
    echo "     -d '{\"prompt\": \"ä½ å¥½\", \"model\": \"qwen2.5:7b\"}'"
    echo ""
    echo "ğŸ”§ é…ç½®æ–‡ä»¶:"
    echo "   - ç¯å¢ƒé…ç½®: .env"
    echo "   - æœåŠ¡ä»£ç : local_llm_proxy.py"
    echo ""
}

# æ¸…ç†å‡½æ•°
cleanup() {
    log_info "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
}

# é”™è¯¯å¤„ç†
error_handler() {
    log_error "éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œæ­£åœ¨æ¸…ç†..."
    cleanup
    exit 1
}

# è®¾ç½®é”™è¯¯å¤„ç†
trap error_handler ERR

# ä¸»ç¨‹åº
main() {
    echo "ğŸš€ å¼€å§‹éƒ¨ç½²æœ¬åœ°å¤§æ¨¡å‹ç³»ç»Ÿ..."
    echo "=================================="

    check_system
    install_dependencies
    install_ollama
    download_models
    install_python_deps
    create_configs
    optimize_system
    start_services

    if run_tests; then
        show_usage
    else
        log_error "éƒ¨ç½²å®Œæˆä½†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"
        exit 1
    fi
}

# å‘½ä»¤è¡Œå‚æ•°å¤„ç†
case "${1:-}" in
    "install")
        main
        ;;
    "start")
        ./start.sh
        ;;
    "stop")
        ./stop.sh
        ;;
    "test")
        ./test.sh
        ;;
    "monitor")
        ./monitor.sh
        ;;
    "clean")
        log_info "æ¸…ç†ç³»ç»Ÿ..."
        ./stop.sh
        rm -f .env start.sh stop.sh test.sh monitor.sh
        log_info "æ¸…ç†å®Œæˆ"
        ;;
    *)
        echo "ç”¨æ³•: $0 {install|start|stop|test|monitor|clean}"
        echo ""
        echo "å‘½ä»¤è¯´æ˜:"
        echo "  install  - ä¸€é”®å®‰è£…å’Œéƒ¨ç½²ç³»ç»Ÿ"
        echo "  start    - å¯åŠ¨æœåŠ¡"
        echo "  stop     - åœæ­¢æœåŠ¡"
        echo "  test     - è¿è¡Œæµ‹è¯•"
        echo "  monitor  - æ€§èƒ½ç›‘æ§"
        echo "  clean    - æ¸…ç†ç³»ç»Ÿ"
        exit 1
        ;;
esac